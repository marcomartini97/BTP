{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI9cUSUTuk6-",
        "outputId": "c78bff9c-afaa-41fa-dade-149131af1e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.10/dist-packages (7.1.1)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from rdflib) (0.7.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install rdflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS8mKYNLvEeo",
        "outputId": "ca9c75e6-7ca2-457d-94fa-22055270f962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datetime in /usr/local/lib/python3.10/dist-packages (5.5)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from datetime) (7.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2024.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (75.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6wKYePVvGrz",
        "outputId": "d502af92-c56a-436b-c731-f93bb9690f04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnbu2RlQvI3Z",
        "outputId": "1a605357-cf77-48c1-8c87-8a6de6881516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"
          ]
        }
      ],
      "source": [
        "pip install psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UdAPwvm7vKfU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "from rdflib import Graph, Literal, RDF, RDFS, URIRef, Namespace\n",
        "from rdflib.plugins.sparql import prepareQuery\n",
        "from rdflib.namespace import XSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "popuhA9jvOPN"
      },
      "outputs": [],
      "source": [
        "# To measure the usage of RAM\n",
        "import psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qohNwW5svO0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e279606b-0cff-4b13-ab36-49f67aa18060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Use your personal account!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JsLnxkCOxZwY"
      },
      "outputs": [],
      "source": [
        "global pbar\n",
        "\n",
        "chunksize = 10000\n",
        "\n",
        "BTP = Namespace(\"http://www.dei.unipd.it/~gdb/ontology/btp/\")\n",
        "\n",
        "global viaChiarini_gp, giardiniMargherita_gp, portaSanFelice_gp\n",
        "viaChiarini_gp = [44.4997732567231, 11.2873095406444]\n",
        "giardiniMargherita_gp = [44.4830615285162, 11.3528830371546] # via Medaro Bottonelli\n",
        "portaSanFelice_gp = [44.4991470592725, 11.3270506316853]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-nvtR9CTvQ3q"
      },
      "outputs": [],
      "source": [
        "# Function to populate the coils dataset\n",
        "def coils_process_chunk(chunk, piece, year_dataset):\n",
        "    # Graph\n",
        "    chunk_set = set()\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        # I check if the record is valid or not -> must have all the field not NaN\n",
        "        if row['Livello'] == '' or row['tipologia'] == '' or row['codice arco'] == '':\n",
        "            # I skip the record -> next record\n",
        "            continue\n",
        "\n",
        "        # else: is valid -> continue\n",
        "\n",
        "        ## COIL:\n",
        "        # -uri: coil_ + id number.\n",
        "        # -attributi: hasID\n",
        "        # -object properties: hasLevel, hasType, isOn, and isPlacedOn.\n",
        "\n",
        "        Coil = \"btp:coil_\"+str(row['ID_univoco_stazione_spira'])\n",
        "\n",
        "        # PollutionCoils and SimpleCoils are subclasses of Coil\n",
        "        chunk_set.add(\"btp:PollutionCoil rdfs:subClassOf btp:Coil .\")\n",
        "        chunk_set.add(\"btp:SimpleCoil rdfs:subClassOf btp:Coil .\")\n",
        "\n",
        "        # Cast to float\n",
        "        latitudine = row['latitudine']\n",
        "        longitudine = row['longitudine']\n",
        "\n",
        "        if(type(latitudine) == str):\n",
        "            latitudine = latitudine.replace(',', '')\n",
        "            # From 113473933293812,00 to 11.3473933293812\n",
        "            latitudine = latitudine[:2] + '.' + latitudine[2:]\n",
        "            # Cast to float\n",
        "            latitudine = float(latitudine)\n",
        "        if(type(longitudine) == str):\n",
        "            longitudine = longitudine.replace(',', '')\n",
        "            # From 44500438455000,00 to 44.500438455000\n",
        "            longitudine = longitudine[:2] + '.' + longitudine[2:]\n",
        "            longitudine = float(longitudine)\n",
        "\n",
        "        # Pollution coils -> must be around 300 m\n",
        "        if ((latitudine <= viaChiarini_gp[0] + 0.0027) and (latitudine >= viaChiarini_gp[0] + 0.0027)) and ((longitudine <= viaChiarini_gp[1] + 0.0013) and (longitudine >= viaChiarini_gp[1] - 0.0013)):\n",
        "            chunk_set.add(Coil + \" a btp:PollutionCoil\")\n",
        "            PollutionStation = \"btp:controlUnitViaChiarini\"\n",
        "            chunk_set.add(PollutionStation + \" a btp:PollutionStation .\")\n",
        "            chunk_set.add(PollutionStation + \" btp:isNearTo \" + Coil +\" .\")\n",
        "        elif ((latitudine <= giardiniMargherita_gp[0] + 0.0027) and (latitudine >= giardiniMargherita_gp[0] + 0.0027)) and ((longitudine <= giardiniMargherita_gp[1] + 0.0013) and (longitudine >= giardiniMargherita_gp[1] - 0.0013)):\n",
        "            chunk_set.add(Coil + \" a btp:PollutionCoil\")\n",
        "            PollutionStation = \"btp:controlUnitGiardiniMargherita\"\n",
        "            chunk_set.add(PollutionStation + \" a btp:PollutionStation .\")\n",
        "            chunk_set.add(PollutionStation + \" btp:isNearTo \" + Coil +\" .\")\n",
        "        elif ((latitudine <= portaSanFelice_gp[0] + 0.0027) and (latitudine >= portaSanFelice_gp[0] + 0.0027)) and ((longitudine <= portaSanFelice_gp[1] + 0.0013) and (longitudine >= portaSanFelice_gp[1] - 0.0013)):\n",
        "            chunk_set.add(Coil + \" a btp:PollutionCoil\")\n",
        "            PollutionStation = \"btp:controlUnitPortaSanFelice\"\n",
        "            chunk_set.add(PollutionStation + \" a btp:PollutionStation .\")\n",
        "            chunk_set.add(PollutionStation + \" btp:isNearTo \" + Coil +\" .\")\n",
        "        else:\n",
        "            chunk_set.add(Coil + \" a btp:SimpleCoil\")\n",
        "\n",
        "\n",
        "        for i in range(2, 26):\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "            VehicleDetection = \"btp:vehicleDetection_\"+str(row['ID_univoco_stazione_spira'])+\"_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+str(i-2).zfill(2)+\":00-\"+str(i-1).zfill(2)+\":00\"\n",
        "            chunk_set.add(VehicleDetection + \" a btp:VehicleDetection .\")\n",
        "            chunk_set.add(VehicleDetection + \" btp:isObserved \" + Coil + \" .\")\n",
        "            chunk_set.add(Coil + \" btp:hasObserve \" + VehicleDetection + \" .\")\n",
        "\n",
        "        Level = \"btp:level_\"+str(int(row['Livello']))\n",
        "        chunk_set.add(Level + \" a btp:Level .\")\n",
        "        chunk_set.add(Coil + \" btp:hasLevel \" + Level + \" .\")\n",
        "\n",
        "        Type = URIRef(BTP[\"type_\"+str(row['tipologia'])])\n",
        "        Type = \"btp:type_\"+str(row['tipologia'])\n",
        "        chunk_set.add(Coil + \" a btp:Type .\")\n",
        "        chunk_set.add(Coil + \" btp:hasType \" + Type + \" .\")\n",
        "\n",
        "        chunk_set.add(Coil + \" btp:hasID \" + Literal(str(row['codice spira']), datatype=XSD.string))\n",
        "\n",
        "        RoadArch = \"btp:road_\"+str(row['codice arco'])\n",
        "        chunk_set.add(Coil + \" a btp:RoadArch .\")\n",
        "        chunk_set.add(Coil + \" btp:isOn \" + RoadArch + \" .\")\n",
        "        chunk_set.add(RoadArch + \" btp:isPlacedOn \" + Coil + \" .\")\n",
        "\n",
        "    pbar.update(len(chunk))\n",
        "\n",
        "    return chunk_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b4X79ZOAMSku"
      },
      "outputs": [],
      "source": [
        "# Function that populates the vehicle count dataset\n",
        "def vehicle_count_process_chunk(chunk, piece, year_dataset):\n",
        "\n",
        "    vc_set = set()\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        # I check if the record is valid or not -> must have all the field not NaN\n",
        "        if row['Livello'] == '' or row['tipologia'] == '':\n",
        "            # I skip the record -> next record\n",
        "            continue\n",
        "        # else: is valid -> continue\n",
        "\n",
        "        for i in range(2, 26):\n",
        "\n",
        "            ## VEHICLEDETECTION:\n",
        "            # -uri: vehicleDetection_ + id number + _ + date.\n",
        "            # -attributi: hasCount.\n",
        "            # -object properties: isObserved, hasObserve, isObservedOnPeriod, and hasObservedOnPeriod.\n",
        "\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "            VehicleDetection = \"btp:vehicleDetection_\"+str(row['ID_univoco_stazione_spira'])+\"_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+str(i-2).zfill(2)+\":00-\"+str(i-1).zfill(2)+\":00\"\n",
        "            vc_set.add(VehicleDetection + \"a btp:VehicleDetection .\")\n",
        "\n",
        "            vc_set.add(VehicleDetection + \"btp:hasCount\" + Literal(row.iloc[i], datatype=XSD.integer) + \" .\")\n",
        "\n",
        "            # # PERIOD:\n",
        "            # -uri: period_ + date + _ + hour1 + _ + hour2.\n",
        "            # -attributi: startTime and endTime.\n",
        "            # -object properties: onDay.\n",
        "\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "            Period = \"period_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+str(i-2).zfill(2)+\":00-\"+str(i-1).zfill(2)+\":00\"\n",
        "            vc_set.add(Period + \" a btp:Period .\")\n",
        "\n",
        "            vc_set.add(Period + \" btp:isObservedOnPeriod \" + VehicleDetection + \" .\")\n",
        "            vc_set.add(VehicleDetection + \" btp:hasObserve \" + Period + \" .\")\n",
        "\n",
        "            startTime = str(i-2).zfill(2)+\":00\"\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "\n",
        "            vc_set.add(Period + \" btp:startTime \" + Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+startTime, datatype=XSD.dateTime) + \" .\")\n",
        "\n",
        "            endTime = str(i-1).zfill(2)+\":00\"\n",
        "\n",
        "            # If the endTime is 24 -> date+1 and endTime = 00\n",
        "            if(endTime == '24:00'):\n",
        "                endTime = '00:00'\n",
        "                # I add one day\n",
        "                date_obj = date_obj + datetime.timedelta(days=1)\n",
        "\n",
        "            vc_set.add(Period + \" btp:endTime \" + Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+endTime, datatype=XSD.dateTime) + \" .\")\n",
        "\n",
        "            ## Convert day from italian to english ex: lunedì -> monday\n",
        "            day_value = ''\n",
        "            if 'Giorno della settimana' in row:\n",
        "                day_value = str(row['Giorno della settimana']).lower()\n",
        "            elif 'giorno della settimana' in row:\n",
        "                day_value = str(row['giorno della settimana']).lower()\n",
        "\n",
        "            match day_value:\n",
        "                case 'lunedì':\n",
        "                    DayWeek = \"btp:Monday\"\n",
        "                    vc_set.add(DayWeek + \" a btp:DayWeek .\")\n",
        "                    vc_set.add(Period + \" btp:onDay \" + DayWeek + \" .\")\n",
        "                case 'martedì':\n",
        "                    DayWeek = \"btp:Tuesday\"\n",
        "                    vc_set.add(DayWeek + \" a btp:DayWeek .\")\n",
        "                    vc_set.add(Period + \" btp:onDay \" + DayWeek + \" .\")\n",
        "                case 'mercoledì':\n",
        "                    DayWeek = \"btp:Wednesday\"\n",
        "                    vc_set.add(DayWeek + \" a btp:DayWeek .\")\n",
        "                    vc_set.add(Period + \" btp:onDay \" + DayWeek + \" .\")\n",
        "                case 'giovedì':\n",
        "                    DayWeek = \"btp:Thursday\"\n",
        "                    vc_set.add(DayWeek + \" a btp:DayWeek .\")\n",
        "                    vc_set.add(Period + \" btp:onDay \" + DayWeek + \" .\")\n",
        "                case 'venerdì':\n",
        "                    DayWeek = \"btp:Friday\"\n",
        "                    vc_set.add(DayWeek + \" a btp:DayWeek .\")\n",
        "                    vc_set.add(Period + \" btp:onDay \" + DayWeek + \" .\")\n",
        "                case 'sabato':\n",
        "                    DayWeek = \"btp:Saturday\"\n",
        "                    vc_set.add(DayWeek + \" a btp:DayWeek .\")\n",
        "                    vc_set.add(Period + \" btp:onDay \" + DayWeek + \" .\")\n",
        "                case 'domenica':\n",
        "                    DayWeek = \"btp:Sunday\"\n",
        "                    vc_set.add(DayWeek + \" a btp:DayWeek .\")\n",
        "                    vc_set.add(Period + \" btp:onDay \" + DayWeek + \" .\")\n",
        "                case _:\n",
        "                    # No day provided\n",
        "                    pass\n",
        "\n",
        "    pbar.update(len(chunk))\n",
        "\n",
        "    return vc_set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that populates the vehicle accuracy dataset\n",
        "def vehicle_accuracy_process_chunk(chunk):\n",
        "\n",
        "    # Graphs\n",
        "    acc_set = set()\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        for i in range(2, 26):\n",
        "\n",
        "            ## VEHICLEDETECTION:\n",
        "            # -uri: vehicleDetection_ + id number + _ + date.\n",
        "            # -attributi: hasAccuracy, and hasCount.\n",
        "\n",
        "            coil = ''\n",
        "\n",
        "            # Query to get the coil's code associated to an ID\n",
        "            coil = get_coil_by_id(str(row['codice spira']))\n",
        "            if coil == '':\n",
        "                # I skip the record -> next record\n",
        "                continue\n",
        "\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "\n",
        "            VehicleDetection = \"btp:vehicleDetection_\"+coil+\"_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+str(i-2).zfill(2)+\":00-\"+str(i-1).zfill(2)+\":00\"\n",
        "            acc_set(VehicleDetection + \" a btp:VehicleDetection .\")\n",
        "            percentage = row.iloc[i].replace('%', '')\n",
        "            acc_set(VehicleDetection + \" btp:hasCount \" + Literal(row.iloc[i], datatype=XSD.integer) + \" .\")\n",
        "\n",
        "            # # PERIOD:\n",
        "            # -uri: period_ + date + _ + hour1 + _ + hour2.\n",
        "            # -attributi: startTime and endTime.\n",
        "            # -object properties: onDay.\n",
        "\n",
        "            Period = \"period_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+str(i-2).zfill(2)+\":00-\"+str(i-1).zfill(2)+\":00\"\n",
        "            acc_set(Period + \" a btp:Period .\")\n",
        "\n",
        "            acc_set(Period + \" btp:isObservedOnPeriod \" + VehicleDetection + \" .\")\n",
        "            acc_set(VehicleDetection + \" btp:hasObservedOnPeriod \" + Period + \" .\")\n",
        "\n",
        "            startTime = str(i-2).zfill(2)+\":00\"\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "\n",
        "            acc_set(Period + \" btp:startTime \" + Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+startTime, datatype=XSD.dateTime) + \" .\")\n",
        "\n",
        "            endTime = str(i-1).zfill(2)+\":00\"\n",
        "\n",
        "            # If the endTime is 24 -> date+1 and endTime = 00\n",
        "            if(endTime == '24:00'):\n",
        "                endTime = '00:00'\n",
        "                # I add one day\n",
        "                date_obj = date_obj + datetime.timedelta(days=1)\n",
        "\n",
        "            acc_set.add(Period + \" btp:endTime \" + Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+endTime, datatype=XSD.dateTime) + \" .\")\n",
        "\n",
        "    pbar.update(len(chunk))"
      ],
      "metadata": {
        "id": "qNfIBNPI_apI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that populates the pollution data\n",
        "def pollution_process_chunk(chunk, piece, year_dataset):\n",
        "\n",
        "    pol_set = set()\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        ## POLLUTIONSTATION:\n",
        "        # -uri: centralUnit + pollution name.\n",
        "        # -object properties: hasRegister, and isRegistered.\n",
        "\n",
        "        PollutionStation = \"controlUnit\" + (str(row['COD_STAZ']).lower()).replace(\" \", \"\")\n",
        "        pol_set.add(PollutionStation + \" a btp:PollutionStation .\")\n",
        "\n",
        "        # PERIOD:\n",
        "        # -uri: period_ + date + _ + hour1 + _ + hour2.\n",
        "        # -attributi: startTime and endTime.\n",
        "        # -object properties: onDay.\n",
        "\n",
        "        # date format: yyyy-mm-ddThh:mm:ss+hh:mm\n",
        "        # keep only the data: 'Thh:mm:ss+hh:mm' -> yyyy-mm-dd\n",
        "        date_obj = datetime.datetime.strptime((str(row['DATA_INIZIO']).split('T'))[0], '%Y-%m-%d')\n",
        "        # keep only the hour: 'Thh:mm:ss+hh:mm' -> hh:mm:ss\n",
        "        startTime = str((((str(row['DATA_INIZIO']).split('T'))[1].split('+')[0]).split(':'))[0])+\":00\"\n",
        "        endTime = str((((str(row['DATA_FINE']).split('T'))[1].split('+')[0]).split(':'))[0])+\":00\"\n",
        "        Period = URIRef(BTP[\"period_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+startTime+\"-\"+endTime])\n",
        "\n",
        "        pol_set.add(Period + \" a btp:Period .\")\n",
        "        pol_set.add(Period + \" btp:startDay \" + Literal(date_obj.strftime('%Y-%m-%d'), datatype=XSD.date) + \" .\")\n",
        "        pol_set.add(Period + \" btp:endTime \" + Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+endTime, datatype=XSD.dateTime) + \" .\")\n",
        "\n",
        "        ## CHEMICALDETECTION:\n",
        "        # -uri: chemicalDetection_ + pollution_station_name + _ + date + _ + element.\n",
        "        # -attributi: inQuantity (conversion all in ug/m), and hasChemicalName.\n",
        "        # -object properties: isDetectedOnPeriod, hasDetectedOnPeriod, hasDetect, and isDetected.\n",
        "\n",
        "        chemical_element = (row['AGENTE'].split(\"(\")[0]).strip()\n",
        "        ChemicalElement = URIRef(BTP[\"chemicalElement_\"+chemical_element])\n",
        "\n",
        "        date_obj = datetime.datetime.strptime((str(row['DATA_INIZIO']).split('T'))[0], '%Y-%m-%d')\n",
        "        ChemicalDetection = \"chemicalDetection_\"+(str(row['COD_STAZ']).lower()).replace(\" \", \"\")+\"_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+startTime+\"-\"+endTime+\"_\"+chemical_element\n",
        "        pol_set(ChemicalDetection + \" a btp:ChemicalDetection .\")\n",
        "\n",
        "        # Cast from mg/m^3 to ug/m^3\n",
        "        if(row['UM'] == 'mg/m3'):\n",
        "            pol_set.add(ChemicalDetection + \" btp:inQuantity \" + Literal((row['VALORE']*1000), datatype=XSD.float) + \" .\")\n",
        "        else:\n",
        "            pol_set.add(ChemicalDetection + \" btp:inQuantity \" + Literal((row['VALORE']), datatype=XSD.float) + \" .\")\n",
        "\n",
        "        ## CHEMICALELEMENT:\n",
        "        # -uri: chemicalElement_ + chemical element name.\n",
        "        # -object properties: hasDetect, and isDetected\n",
        "\n",
        "        pol_set.add(ChemicalElement + \" a btp:ChemicalElement .\")\n",
        "        pol_set.add(ChemicalDetection + \" btp:hasDetected \" + ChemicalElement + \" .\")\n",
        "        pol_set.add(ChemicalElement + \" btp:isDetected \" + ChemicalDetection + \" .\")\n",
        "\n",
        "        if len(row['AGENTE'].split(\"(\")) > 1:\n",
        "            chemical_element_name = (((row['AGENTE'].split(\"(\")[1]).replace(\")\",\"\")).strip()).lower()\n",
        "\n",
        "            match chemical_element_name:\n",
        "                case 'benzene':\n",
        "                    pol_set.add(ChemicalDetection + \" btp:hasChemicalName \" + Literal(\"Benzene\", datatype=XSD.string) + \" .\")\n",
        "                case 'monossido di carbonio':\n",
        "                    pol_set.add(ChemicalDetection + \" btp:hasChemicalName \" + Literal(\"Carbon monoxide\", datatype=XSD.string) + \" .\")\n",
        "                case 'monossido di azoto':\n",
        "                    pol_set.add(ChemicalDetection + \" btp:hasChemicalName \" + Literal(\"Nitrogen Monoxide\", datatype=XSD.string) + \" .\")\n",
        "                case 'biossido di azoto':\n",
        "                    pol_set.add(ChemicalDetection + \" btp:hasChemicalName \" + Literal(\"Nitrogen dioxide\", datatype=XSD.string) + \" .\")\n",
        "                case 'ossidi di azoto':\n",
        "                    pol_set.add(ChemicalDetection + \" btp:hasChemicalName \" + Literal(\"Nitrogen oxides\", datatype=XSD.string) + \" .\")\n",
        "                case 'ozono':\n",
        "                    pol_set.add(ChemicalDetection + \" btp:hasChemicalName \" + Literal(\"Ozone\", datatype=XSD.string) + \" .\")\n",
        "                case _:\n",
        "                    # New element provided\n",
        "                    pol_set.add(ChemicalDetection + \" btp:hasChemicalName \" + Literal(chemical_element_name, datatype=XSD.string) + \" .\")\n",
        "\n",
        "    pbar.update(len(chunk))"
      ],
      "metadata": {
        "id": "BG0Zbtji_Wan"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yKKpAEOJvkXc"
      },
      "outputs": [],
      "source": [
        "# Function to save a graph\n",
        "def save_graph(set, path):\n",
        "\n",
        "    with open(path, 'w') as file:\n",
        "\n",
        "        file.write('@prefix btp: ' + BTP + ' .\\n')\n",
        "        file.write('@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\\n')\n",
        "        file.write('@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n')\n",
        "\n",
        "        for elem in set:\n",
        "            file.write(elem)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_coil_by_id(coil_id):\n",
        "    # Graph\n",
        "    g_coils = Graph()\n",
        "    g_coils.parse('/content/coils_populated.ttl', format='turtle')\n",
        "\n",
        "    code_coil_query = prepareQuery(\"\"\"\n",
        "    SELECT DISTINCT ?coil WHERE {\n",
        "        ?coil btp:hasID ?id .\n",
        "    FILTER (?id = ?coil_id)\n",
        "                               }\"\"\" , initNs={\"btp\": BTP})\n",
        "\n",
        "    res = g_coils.query(code_coil_query, initBindings={'coil_id':Literal(coil_id, datatype=XSD.string)})\n",
        "    if res == [] or res == None:\n",
        "        return ''\n",
        "    else:\n",
        "        for r in res:\n",
        "            return str(r.coil).replace('http://www.dei.unipd.it/~gdb/ontology/btp/coil_', '')"
      ],
      "metadata": {
        "id": "2AKBbNI1_cFq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cc3JSytzvaTk"
      },
      "outputs": [],
      "source": [
        "## Datasets\n",
        "\n",
        "# Rilevazione flusso datasets\n",
        "rilevazione_flusso = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/rilevazione_flusso_veicoli_2019.csv')\n",
        "\n",
        "rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2019.csv')\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2020.csv')\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2021.csv')\n",
        "rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2022.csv')\n",
        "\n",
        "# Accuratezza spire datasets\n",
        "accuratezza_spire = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/accuratezza_spire_2019.csv')\n",
        "\n",
        "accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2019.csv')\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2020.csv')\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2021.csv')\n",
        "accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2022.csv')\n",
        "\n",
        "# Centraline qualità datasets\n",
        "centraline = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/dati_centraline_2019.csv')\n",
        "\n",
        "centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2019.csv')\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2020.csv')\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2021.csv')\n",
        "centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2022.csv')\n",
        "\n",
        "# Save path\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/Graph Database/rdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dYvPAPAzvglq"
      },
      "outputs": [],
      "source": [
        "# I check if the folder is empty or not\n",
        "if not os.listdir(save_path) == []:\n",
        "    print(\"The folder is not empty, do you want to continue? (y/n)\")\n",
        "    answer = input()\n",
        "    if(answer.lower() == 'y'):\n",
        "        # I remove all the files in the folder\n",
        "        print(\"Removing all the files in the folder ...\")\n",
        "        for file in os.listdir(save_path):\n",
        "            os.remove(os.path.join(save_path, file))\n",
        "        print(\"DONE!\")\n",
        "    else:\n",
        "        exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8xpGnlBv019",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19dd2424-f61e-431c-f504-5febee262355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- populating coils ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 10000/287747 [00:17<07:56, 583.30it/s]\n",
            "  0%|          | 0/302872 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "print(\"--- populating coils ---\")\n",
        "\n",
        "coils_set = set()\n",
        "\n",
        "for namefile in rilevazione_flusso:\n",
        "\n",
        "    year_dataset = namefile.split('_')[3].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        coils_set.update(coils_process_chunk(chunk, piece, year_dataset))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(coils_set, '/content/coils_populated_'+year_dataset+'_'+str(piece)+'.txt')\n",
        "            # Reset the set\n",
        "            coils_set.clear()\n",
        "            piece += 1\n",
        "        break\n",
        "\n",
        "    save_graph(coils_set, '/content/coils_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "    coils_set.clear()\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As2AF1eCwUrG"
      },
      "outputs": [],
      "source": [
        "# Free memory\n",
        "del coils_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEjxAUd2Muhc"
      },
      "outputs": [],
      "source": [
        "print(\"--- populating coils ---\")\n",
        "\n",
        "vehicle_count_set = set()\n",
        "\n",
        "for namefile in rilevazione_flusso:\n",
        "\n",
        "    break\n",
        "\n",
        "    year_dataset = namefile.split('_')[3].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        vehicle_count_set.update(vehicle_count_process_chunk(chunk, piece, year_dataset))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(vehicle_count_set, '/content/vehicle_count_populated_'+year_dataset+'_'+str(piece)+'.txt')\n",
        "            # Reset the set\n",
        "            vehicle_count_set.clear()\n",
        "            piece += 1\n",
        "\n",
        "    save_graph(vehicle_count_set, '/content/vehicle_count_populated_'+year_dataset+'_'+str(piece)+'.txt')\n",
        "    vehicle_count_set.clear()\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gulLdqXsM_xm"
      },
      "outputs": [],
      "source": [
        "# Free memory\n",
        "del vehicle_count_set"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- populating vehicle accuracy ---\")\n",
        "\n",
        "acc_set = set()\n",
        "\n",
        "for namefile in rilevazione_flusso:\n",
        "\n",
        "    year_dataset = namefile.split('_')[3].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        acc_set.update(coils_process_chunk(chunk, piece, year_dataset))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(coils_set, '/content/coils_populated_'+year_dataset+'_'+str(piece)+'.txt')\n",
        "            # Reset the set\n",
        "            acc_set.clear()\n",
        "            piece += 1\n",
        "        break\n",
        "\n",
        "    save_graph(coils_set, '/content/coils_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "    acc_set.clear()\n",
        "\n",
        "    pbar.close()"
      ],
      "metadata": {
        "id": "Pdf4EARn_p4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free memory\n",
        "del acc_set"
      ],
      "metadata": {
        "id": "MwKPMCZB_q41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- populating pollution data ---\")\n",
        "\n",
        "pollution_set = set()\n",
        "\n",
        "for namefile in rilevazione_flusso:\n",
        "\n",
        "    year_dataset = namefile.split('_')[3].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        pollution_set.update(pollution_process_chunk(chunk, piece, year_dataset))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(coils_set, '/content/pollution_populated_'+year_dataset+'_'+str(piece)+'.txt')\n",
        "            # Reset the set\n",
        "            pollution_set.clear()\n",
        "            piece += 1\n",
        "        break\n",
        "\n",
        "    save_graph(coils_set, '/content/pollution_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "    pollution_set.clear()\n",
        "\n",
        "    pbar.close()"
      ],
      "metadata": {
        "id": "uNf5npVF_rM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Free memory\n",
        "del pollution_set"
      ],
      "metadata": {
        "id": "OcS0-7v6AFWw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}