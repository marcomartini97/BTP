{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI9cUSUTuk6-",
        "outputId": "419f08cf-8249-447c-92a8-ce1321ce825e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.10/dist-packages (7.1.1)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from rdflib) (0.7.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install rdflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS8mKYNLvEeo",
        "outputId": "a2b03816-5453-43fd-8004-fd412f91723b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datetime in /usr/local/lib/python3.10/dist-packages (5.5)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from datetime) (7.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2024.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (75.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6wKYePVvGrz",
        "outputId": "f12384bd-7925-4b99-ff33-6e8b9504b6f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnbu2RlQvI3Z",
        "outputId": "b99b59f4-aa02-44e7-d8fc-c0807f23fa15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"
          ]
        }
      ],
      "source": [
        "pip install psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UdAPwvm7vKfU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "from rdflib import Graph, Literal, RDF, RDFS, URIRef, Namespace\n",
        "from rdflib.plugins.sparql import prepareQuery\n",
        "from rdflib.namespace import XSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "popuhA9jvOPN"
      },
      "outputs": [],
      "source": [
        "# To measure the usage of RAM\n",
        "import psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qohNwW5svO0d",
        "outputId": "1ad4f600-469a-4c59-bc8c-a720422ea3be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Use your personal account!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JsLnxkCOxZwY"
      },
      "outputs": [],
      "source": [
        "global pbar\n",
        "\n",
        "chunksize = 10000\n",
        "\n",
        "BTP = Namespace('http://www.dei.unipd.it/~gdb/ontology/btp/')\n",
        "\n",
        "global viaChiarini_gp, giardiniMargherita_gp, portaSanFelice_gp\n",
        "viaChiarini_gp = [44.4997732567231, 11.2873095406444]\n",
        "giardiniMargherita_gp = [44.4830615285162, 11.3528830371546] # via Medaro Bottonelli\n",
        "portaSanFelice_gp = [44.4991470592725, 11.3270506316853]\n",
        "\n",
        "global coil_dict\n",
        "coil_dict = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-nvtR9CTvQ3q"
      },
      "outputs": [],
      "source": [
        "# Function to populate the coils dataset\n",
        "def coils_process_chunk(chunk : set) -> set:\n",
        "    # Graph\n",
        "    chunk_set = set()\n",
        "\n",
        "    # OWL - Object Properties\n",
        "    chunk_set.add(':isNearTo a owl:ObjectProperty .')\n",
        "    chunk_set.add(':isObserved a owl:ObjectProperty .')\n",
        "    chunk_set.add(':hasObserve a owl:ObjectProperty .')\n",
        "    chunk_set.add(':hasLevel a owl:ObjectProperty .')\n",
        "    chunk_set.add(':hasType a owl:ObjectProperty .')\n",
        "    chunk_set.add(':isOn a owl:ObjectProperty .')\n",
        "    chunk_set.add(':isPlaced a owl:ObjectProperty .')\n",
        "\n",
        "    # OWL - DataType Properties\n",
        "    chunk_set.add(':hasID a owl:DatatypeProperty .')\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        # I check if the record is valid or not -> must have all the field not NaN\n",
        "        if row['Livello'] == '' or row['tipologia'] == '' or row['codice arco'] == '' or row['ID_univoco_stazione_spira'] == '':\n",
        "            # I skip the record -> next record\n",
        "            continue\n",
        "\n",
        "        # else: is valid -> continue\n",
        "\n",
        "        ## COIL:\n",
        "        # -uri: coil_ + id number.\n",
        "        # -attributi: hasID\n",
        "        # -object properties: hasLevel, hasType, isOn, and isPlaced.\n",
        "\n",
        "        Coil = ':coil_'+str(int(row['ID_univoco_stazione_spira']))\n",
        "\n",
        "        # PollutionCoils and SimpleCoils are subclasses of Coil\n",
        "        chunk_set.add(':PollutionCoil rdfs:subClassOf :Coil .')\n",
        "        chunk_set.add(':SimpleCoil rdfs:subClassOf :Coil .')\n",
        "\n",
        "        # Cast to float\n",
        "        latitudine = row['latitudine']\n",
        "        longitudine = row['longitudine']\n",
        "\n",
        "        if(type(latitudine) == str):\n",
        "            latitudine = latitudine.replace(',', '')\n",
        "            # From 113473933293812,00 to 11.3473933293812\n",
        "            latitudine = latitudine[:2] + '.' + latitudine[2:]\n",
        "            # Cast to float\n",
        "            latitudine = float(latitudine)\n",
        "        if(type(longitudine) == str):\n",
        "            longitudine = longitudine.replace(',', '')\n",
        "            # From 44500438455000,00 to 44.500438455000\n",
        "            longitudine = longitudine[:2] + '.' + longitudine[2:]\n",
        "            longitudine = float(longitudine)\n",
        "\n",
        "        # Pollution coils -> must be around 300 m\n",
        "        if ((viaChiarini_gp[0] - 0.0027 <= latitudine <= viaChiarini_gp[0] + 0.0027) and (viaChiarini_gp[1] - 0.0013 <= longitudine <= viaChiarini_gp[1] + 0.0013)):\n",
        "            chunk_set.add(Coil + ' a :PollutionCoil .')\n",
        "            PollutionStation = ':viaChiariniControlUnit'\n",
        "            chunk_set.add(PollutionStation + ' a :PollutionStation .')\n",
        "            chunk_set.add(PollutionStation + ' :isNearTo ' + Coil +' .')\n",
        "        elif ((giardiniMargherita_gp[0] - 0.0027 <= latitudine <= giardiniMargherita_gp[0] + 0.0027) and (giardiniMargherita_gp[1] - 0.0013 <= longitudine <= giardiniMargherita_gp[1] + 0.0013)):\n",
        "            chunk_set.add(Coil + ' a :PollutionCoil .')\n",
        "            PollutionStation = ':giardiniMargheritaControlUnit'\n",
        "            chunk_set.add(PollutionStation + ' a :PollutionStation .')\n",
        "            chunk_set.add(PollutionStation + ' :isNearTo ' + Coil +' .')\n",
        "        elif ((portaSanFelice_gp[0] - 0.0027 <= latitudine <= portaSanFelice_gp[0] + 0.0027) and (portaSanFelice_gp[1] - 0.0013 <= longitudine <= portaSanFelice_gp[1] + 0.0013)):\n",
        "            chunk_set.add(Coil + ' a :PollutionCoil .')\n",
        "            PollutionStation = ':portaSanFeliceControlUnit'\n",
        "            chunk_set.add(PollutionStation + ' a :PollutionStation .')\n",
        "            chunk_set.add(PollutionStation + ' :isNearTo ' + Coil +' .')\n",
        "        else:\n",
        "            chunk_set.add(Coil + ' a :SimpleCoil .')\n",
        "\n",
        "\n",
        "        for i in range(2, 26):\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "            VehicleDetection = ':veDet_'+str(int(row['ID_univoco_stazione_spira']))+'_'+(date_obj.strftime('%Y-%m-%d')).replace('-', '_')+'_'+str(i-2).zfill(2)+'_'+str(i-1).zfill(2)\n",
        "            chunk_set.add(VehicleDetection + ' a :VehicleDetection .')\n",
        "            chunk_set.add(VehicleDetection + ' :isObserved ' + Coil + ' .')\n",
        "            chunk_set.add(Coil + ' :hasObserve ' + VehicleDetection + ' .')\n",
        "\n",
        "        Level = ':level'+str(int(row['Livello']))\n",
        "        chunk_set.add(Level + ' a :Level .')\n",
        "        chunk_set.add(Coil + ' :hasLevel ' + Level + ' .')\n",
        "\n",
        "        Type = URIRef(BTP['type_'+str(row['tipologia'])])\n",
        "        Type = ':'+str(row['tipologia'])\n",
        "        chunk_set.add(Type + ' a :Type .')\n",
        "        chunk_set.add(Coil + ' :hasType ' + Type + ' .')\n",
        "\n",
        "        chunk_set.add(Coil + ' :hasID \"' + str(row['codice spira']) + '\"^^xsd:string .')\n",
        "\n",
        "        RoadArch = ':roadarch_'+str(int(row['codice arco']))\n",
        "        chunk_set.add(RoadArch + ' a :RoadArch .')\n",
        "        chunk_set.add(Coil + ' :isOn ' + RoadArch + ' .')\n",
        "        chunk_set.add(RoadArch + ' :isPlaced ' + Coil + ' .')\n",
        "\n",
        "        # Update the dictionary\n",
        "        coil_dict[str(row['codice spira'])] = str(int(row['ID_univoco_stazione_spira']))\n",
        "\n",
        "    pbar.update(len(chunk))\n",
        "\n",
        "    return chunk_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b4X79ZOAMSku"
      },
      "outputs": [],
      "source": [
        "# Function that populates the vehicle count dataset\n",
        "def vehicle_count_process_chunk(chunk: set) -> set:\n",
        "\n",
        "    vc_set = set()\n",
        "\n",
        "    # OWL - Object Properties\n",
        "    vc_set.add(':isObservedOnPeriod a owl:ObjectProperty .')\n",
        "    vc_set.add(':hasObservedOnPeriod a owl:ObjectProperty .')\n",
        "    vc_set.add(':onDay a owl:ObjectProperty .')\n",
        "\n",
        "    # OWL - DataType Properties\n",
        "    vc_set.add(':hasCount a owl:DatatypeProperty .')\n",
        "    vc_set.add(':startTime a owl:DatatypeProperty .')\n",
        "    vc_set.add(':endTime a owl:DatatypeProperty .')\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        # I check if the record is valid or not -> must have all the field not NaN\n",
        "        if row['Livello'] == '' or row['tipologia'] == '' or row['ID_univoco_stazione_spira'] == '':\n",
        "            # I skip the record -> next record\n",
        "            continue\n",
        "        # else: is valid -> continue\n",
        "\n",
        "        for i in range(2, 26):\n",
        "\n",
        "            ## VEHICLEDETECTION:\n",
        "            # -uri: vehicleDetection_ + id number + _ + date.\n",
        "            # -attributi: hasCount.\n",
        "            # -object properties: isObserved, hasObserve, isObservedOnPeriod, and hasObservedOnPeriod.\n",
        "\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "            VehicleDetection = ':veDet_'+str(int(row['ID_univoco_stazione_spira']))+'_'+(date_obj.strftime('%Y-%m-%d')).replace('-', '_')+'_'+str(i-2).zfill(2)+'_'+str(i-1).zfill(2)\n",
        "            vc_set.add(VehicleDetection + ' a :VehicleDetection .')\n",
        "\n",
        "            vc_set.add(VehicleDetection + ' :hasCount \"' + str(int(row.iloc[i])) + '\"^^xsd:integer .')\n",
        "\n",
        "            # # PERIOD:\n",
        "            # -uri: period_ + date + _ + hour1 + _ + hour2.\n",
        "            # -attributi: startTime and endTime.\n",
        "            # -object properties: onDay.\n",
        "\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "            Period = ':period_'+(date_obj.strftime('%Y-%m-%d')).replace('-', '_')+'_'+str(i-2).zfill(2)+'_'+str(i-1).zfill(2)\n",
        "            vc_set.add(Period + ' a :Period .')\n",
        "\n",
        "            vc_set.add(Period + ' :isObservedOnPeriod ' + VehicleDetection + ' .')\n",
        "            vc_set.add(VehicleDetection + ' :hasObservedOnPeriod ' + Period + ' .')\n",
        "\n",
        "            startTime = str(i-2).zfill(2)+':00:00'\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "\n",
        "            vc_set.add(Period + ' :startTime \"' + str(date_obj.strftime('%Y-%m-%d')+'T'+startTime) + '\"^^xsd:dateTime .')\n",
        "\n",
        "            endTime = str(i-1).zfill(2)+':00:00'\n",
        "\n",
        "            # If the endTime is 24 -> date+1 and endTime = 00\n",
        "            if(endTime == '24:00:00'):\n",
        "                endTime = '00:00:00'\n",
        "                # I add one day\n",
        "                date_obj = date_obj + datetime.timedelta(days=1)\n",
        "\n",
        "            vc_set.add(Period + ' :endTime \"' + str(date_obj.strftime('%Y-%m-%d')+'T'+endTime) + '\"^^xsd:dateTime .')\n",
        "\n",
        "            ## Convert day from italian to english ex: lunedì -> monday\n",
        "            day_value = ''\n",
        "            if 'Giorno della settimana' in row:\n",
        "                day_value = str(row['Giorno della settimana']).lower()\n",
        "            elif 'giorno della settimana' in row:\n",
        "                day_value = str(row['giorno della settimana']).lower()\n",
        "\n",
        "            match day_value:\n",
        "                case 'lunedì':\n",
        "                    DayWeek = ':monday'\n",
        "                    vc_set.add(DayWeek + ' a :DayWeek .')\n",
        "                    vc_set.add(Period + ' :onDay ' + DayWeek + ' .')\n",
        "                case 'martedì':\n",
        "                    DayWeek = ':tuesday'\n",
        "                    vc_set.add(DayWeek + ' a :DayWeek .')\n",
        "                    vc_set.add(Period + ' :onDay ' + DayWeek + ' .')\n",
        "                case 'mercoledì':\n",
        "                    DayWeek = ':wednesday'\n",
        "                    vc_set.add(DayWeek + ' a :DayWeek .')\n",
        "                    vc_set.add(Period + ' :onDay ' + DayWeek + ' .')\n",
        "                case 'giovedì':\n",
        "                    DayWeek = ':thursday'\n",
        "                    vc_set.add(DayWeek + ' a :DayWeek .')\n",
        "                    vc_set.add(Period + ' :onDay ' + DayWeek + ' .')\n",
        "                case 'venerdì':\n",
        "                    DayWeek = ':friday'\n",
        "                    vc_set.add(DayWeek + ' a :DayWeek .')\n",
        "                    vc_set.add(Period + ' :onDay ' + DayWeek + ' .')\n",
        "                case 'sabato':\n",
        "                    DayWeek = ':saturday'\n",
        "                    vc_set.add(DayWeek + ' a :DayWeek .')\n",
        "                    vc_set.add(Period + ' :onDay ' + DayWeek + ' .')\n",
        "                case 'domenica':\n",
        "                    DayWeek = ':sunday'\n",
        "                    vc_set.add(DayWeek + ' a :DayWeek .')\n",
        "                    vc_set.add(Period + ' :onDay ' + DayWeek + ' .')\n",
        "                case _:\n",
        "                    # No day provided\n",
        "                    pass\n",
        "\n",
        "    pbar.update(len(chunk))\n",
        "\n",
        "    return vc_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qNfIBNPI_apI"
      },
      "outputs": [],
      "source": [
        "# Function that populates the vehicle accuracy dataset\n",
        "def vehicle_accuracy_process_chunk(chunk : set) -> set:\n",
        "\n",
        "    # Graphs\n",
        "    acc_set = set()\n",
        "\n",
        "    # OWL - DataType Properties\n",
        "    acc_set.add(':hasAccuracy a owl:DatatypeProperty .')\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        for i in range(2, 26):\n",
        "\n",
        "            ## VEHICLEDETECTION:\n",
        "            # -uri: vehicleDetection_ + id number + _ + date.\n",
        "            # -attributi: hasAccuracy, and hasCount.\n",
        "\n",
        "            # Query to get the coil's code associated to an ID\n",
        "            coil = get_coil_by_id(str(row['codice spira']))\n",
        "            if coil == None:\n",
        "                # I skip the record -> next record\n",
        "                continue\n",
        "\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "\n",
        "            VehicleDetection = ':veDet_' + str(int(coil)) + '_' + (str(date_obj.strftime('%Y-%m-%d'))).replace('-', '_') + '_'+str(i-2).zfill(2) + '_' + str(i-1).zfill(2)\n",
        "            acc_set.add(VehicleDetection + ' a :VehicleDetection .')\n",
        "            percentage = row.iloc[i].replace('%', '')\n",
        "            acc_set.add(VehicleDetection + ' :hasAccuracy \"' + str(float(percentage)) + '\"^^xsd:float .')\n",
        "\n",
        "    pbar.update(len(chunk))\n",
        "\n",
        "    return acc_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BG0Zbtji_Wan"
      },
      "outputs": [],
      "source": [
        "# Function that populates the pollution data\n",
        "def pollution_process_chunk(chunk: set) -> set:\n",
        "\n",
        "    pol_set = set()\n",
        "\n",
        "    # OWL - Object Properties\n",
        "    pol_set.add(':hasDetect a owl:ObjectProperty .')\n",
        "    pol_set.add(':isDetected a owl:ObjectProperty .')\n",
        "    pol_set.add(':isRegistered a owl:ObjectProperty .')\n",
        "    pol_set.add(':hasRegister a owl:ObjectProperty .')\n",
        "    pol_set.add(':isDetectedOnPeriod a owl:ObjectProperty .')\n",
        "    pol_set.add(':hasDetectedOnPeriod a owl:ObjectProperty .')\n",
        "\n",
        "    # OWL - DateType Properties\n",
        "    pol_set.add(':startTime a owl:DatatypeProperty .')\n",
        "    pol_set.add(':endTime a owl:DatatypeProperty .')\n",
        "    pol_set.add(':inQuantity a owl:DatatypeProperty .')\n",
        "    pol_set.add(':hasChemicalName a owl:DatatypeProperty .')\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        ## POLLUTIONSTATION:\n",
        "        # -uri: centralUnit + pollution name.\n",
        "        # -object properties: hasRegister, and isRegistered.\n",
        "\n",
        "        PollutionStation = ':' + ((str(row['COD_STAZ']).split(\" \"))[0]).lower() + ''.join(s.capitalize() for s in (str(row['COD_STAZ']).split(\" \"))[1:]) + 'ControlUnit'\n",
        "        pol_set.add(PollutionStation + ' a :PollutionStation .')\n",
        "\n",
        "        # PERIOD:\n",
        "        # -uri: period_ + date + _ + hour1 + _ + hour2.\n",
        "        # -attributi: startTime and endTime.\n",
        "        # -object properties: onDay.\n",
        "\n",
        "        # date format: yyyy-mm-ddThh:mm:ss+hh:mm\n",
        "        # keep only the data: 'Thh:mm:ss+hh:mm' -> yyyy-mm-dd\n",
        "        date_obj = datetime.datetime.strptime((str(row['DATA_INIZIO']).split('T'))[0], '%Y-%m-%d')\n",
        "        # keep only the hour: 'Thh:mm:ss+hh:mm' -> hh:mm:ss\n",
        "        startTime = str((((str(row['DATA_INIZIO']).split('T'))[1].split('+')[0]).split(':'))[0])\n",
        "        endTime = str((((str(row['DATA_FINE']).split('T'))[1].split('+')[0]).split(':'))[0])\n",
        "\n",
        "        Period = ':period_'+(str(date_obj.strftime('%Y-%m-%d'))).replace('-', '_')+'_'+startTime+'_'+endTime\n",
        "        pol_set.add(Period + ' a :Period .')\n",
        "\n",
        "        chemical_element = ((row['AGENTE'].split('(')[0]).strip()).upper()\n",
        "        date_obj = datetime.datetime.strptime((str(row['DATA_INIZIO']).split('T'))[0], '%Y-%m-%d')\n",
        "\n",
        "        ## CHEMICALDETECTION:\n",
        "        # -uri: chemicalDetection_ + pollution_station_name + _ + date + _ + element.\n",
        "        # -attributi: inQuantity (conversion all in ug/m), and hasChemicalName.\n",
        "        # -object properties: isDetectedOnPeriod, hasDetectedOnPeriod, hasDetect, and isDetected.\n",
        "\n",
        "        ChemicalDetection = ':chDet_'+(str(row['COD_STAZ']).lower()).replace(' ', '')+'_'+(str(date_obj.strftime('%Y-%m-%d'))).replace('-', '_')+'_'+startTime+'_'+endTime+'_'+chemical_element\n",
        "        pol_set.add(ChemicalDetection + ' a :ChemicalDetection .')\n",
        "        pol_set.add(ChemicalDetection + ' :isRegistered ' + PollutionStation + ' .')\n",
        "        pol_set.add(PollutionStation + ' :hasRegister ' + ChemicalDetection + ' .')\n",
        "\n",
        "        pol_set.add(Period + ' :isDetectedOnPeriod ' + ChemicalDetection + ' .')\n",
        "        pol_set.add(ChemicalDetection + ' :hasDetectedOnPeriod ' + Period + ' .')\n",
        "\n",
        "        # Cast from mg/m^3 to ug/m^3\n",
        "        if(row['UM'] == 'mg/m3'):\n",
        "            pol_set.add(ChemicalDetection + ' :inQuantity \"' + str(float(row['VALORE']*1000)) + '\"^^xsd:float .')\n",
        "        else:\n",
        "            pol_set.add(ChemicalDetection + ' :inQuantity \"' + str(float(row['VALORE'])) + '\"^^xsd:float .')\n",
        "\n",
        "        ## CHEMICALELEMENT:\n",
        "        # -uri: chemicalElement_ + chemical element name.\n",
        "        # -object properties: hasDetect, and isDetected\n",
        "\n",
        "        ChemicalElement = ':'+chemical_element\n",
        "        pol_set.add(ChemicalElement + ' a :ChemicalElement .')\n",
        "        pol_set.add(ChemicalDetection + ' :hasDetect ' + ChemicalElement + ' .')\n",
        "        pol_set.add(ChemicalElement + ' :isDetected ' + ChemicalDetection + ' .')\n",
        "\n",
        "        if len(row['AGENTE'].split('(')) > 1:\n",
        "            chemical_element_name = (((row['AGENTE'].split('(')[1]).replace(')','')).strip()).lower()\n",
        "\n",
        "            match chemical_element_name:\n",
        "                case 'benzene':\n",
        "                    pol_set.add(ChemicalDetection + ' :hasChemicalName \"Benzene\"^^xsd:string .')\n",
        "                case 'monossido di carbonio':\n",
        "                    pol_set.add(ChemicalDetection + ' :hasChemicalName \"Carbon monoxide\"^^xsd:string .')\n",
        "                case 'monossido di azoto':\n",
        "                    pol_set.add(ChemicalDetection + ' :hasChemicalName \"Nitrogen Monoxide\"^^xsd:string .')\n",
        "                case 'biossido di azoto':\n",
        "                    pol_set.add(ChemicalDetection + ' :hasChemicalName \"Nitrogen dioxide\"^^xsd:string .')\n",
        "                case 'ossidi di azoto':\n",
        "                    pol_set.add(ChemicalDetection + ' :hasChemicalName \"Nitrogen oxides\"^^xsd:string .')\n",
        "                case 'ozono':\n",
        "                    pol_set.add(ChemicalDetection + ' :hasChemicalName \"Ozone\"^^xsd:string .')\n",
        "                case _:\n",
        "                    # New element provided\n",
        "                    pol_set.add(ChemicalDetection + ' :hasChemicalName \"' + chemical_element_name + '\"^^xsd:string .')\n",
        "\n",
        "        startTime = startTime+':00:00'\n",
        "        endTime = endTime+':00:00'\n",
        "\n",
        "        pol_set.add(Period + ' :startTime \"' + str(date_obj.strftime('%Y-%m-%d')+'T'+startTime) + '\"^^xsd:dateTime .')\n",
        "        pol_set.add(Period + ' :endTime \"' + str(date_obj.strftime('%Y-%m-%d')+'T'+endTime) + '\"^^xsd:dateTime .')\n",
        "\n",
        "    pbar.update(len(chunk))\n",
        "\n",
        "    return pol_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yKKpAEOJvkXc"
      },
      "outputs": [],
      "source": [
        "# Function to save a graph\n",
        "def save_graph(set : set, path : str):\n",
        "\n",
        "    with open(path, 'w', encoding=\"utf-8\") as file:\n",
        "\n",
        "        file.write('@prefix : <' + BTP + '> .\\n')\n",
        "        file.write('@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\\n')\n",
        "        file.write('@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n')\n",
        "        file.write('@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n')\n",
        "        file.write('@prefix owl: <http://www.w3.org/2002/07/owl#> .\\n')\n",
        "\n",
        "        file.write('\\n')\n",
        "\n",
        "        for elem in set:\n",
        "            file.write(elem + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2AKBbNI1_cFq"
      },
      "outputs": [],
      "source": [
        "def get_coil_by_id(coil_id : str) -> str:\n",
        "\n",
        "    return coil_dict.get(coil_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cc3JSytzvaTk"
      },
      "outputs": [],
      "source": [
        "## Datasets\n",
        "\n",
        "# Rilevazione flusso datasets\n",
        "rilevazione_flusso = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/rilevazione_flusso_veicoli_2019.csv')\n",
        "\n",
        "rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2019.csv')\n",
        "rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2020.csv')\n",
        "rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2021.csv')\n",
        "rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2022.csv')\n",
        "\n",
        "# Accuratezza spire datasets\n",
        "accuratezza_spire = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/accuratezza_spire_2019.csv')\n",
        "\n",
        "accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2019.csv')\n",
        "accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2020.csv')\n",
        "accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2021.csv')\n",
        "accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2022.csv')\n",
        "\n",
        "# Centraline qualità datasets\n",
        "centraline = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/dati_centraline_2019.csv')\n",
        "\n",
        "centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2019.csv')\n",
        "centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2020.csv')\n",
        "centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2021.csv')\n",
        "centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2022.csv')\n",
        "\n",
        "# Save path\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/Graph Database/rdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8xpGnlBv019",
        "outputId": "147bd21e-3d8d-4651-9c92-780feaf86ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- populating coils ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 287747/287747 [05:02<00:00, 951.06it/s] \n",
            "100%|██████████| 284787/284787 [05:08<00:00, 922.66it/s]\n",
            "100%|██████████| 290531/290531 [04:46<00:00, 1013.45it/s]\n",
            "100%|██████████| 302872/302872 [05:19<00:00, 947.01it/s] \n"
          ]
        }
      ],
      "source": [
        "print('--- populating coils ---')\n",
        "\n",
        "coils_set = set()\n",
        "\n",
        "# Regular expression\n",
        "re_a_type_pol = re.compile(r' a :PollutionStation .')\n",
        "re_a_type_sm = re.compile(r' a :SimpleCoil .')\n",
        "re_hasID = re.compile(r' :hasID ')\n",
        "\n",
        "for namefile in rilevazione_flusso:\n",
        "\n",
        "    year_dataset = namefile.split('_')[3].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        coils_set.update(coils_process_chunk(chunk))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(coils_set, '/content/coils_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "            # Reset the set\n",
        "            coils_set.clear()\n",
        "            piece += 1\n",
        "\n",
        "    save_graph(coils_set, '/content/coils_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "\n",
        "    coils_set.clear()\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "As2AF1eCwUrG"
      },
      "outputs": [],
      "source": [
        "# Free memory\n",
        "del coils_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iEjxAUd2Muhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318d5874-d430-49ec-bf86-eb92a0c44b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- populating coils ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 287747/287747 [12:17<00:00, 390.10it/s]\n",
            "100%|██████████| 284787/284787 [12:47<00:00, 371.09it/s]\n",
            "100%|██████████| 290531/290531 [13:12<00:00, 366.45it/s]\n",
            "100%|██████████| 302872/302872 [14:32<00:00, 347.10it/s]\n"
          ]
        }
      ],
      "source": [
        "print('--- populating coils ---')\n",
        "\n",
        "vehicle_count_set = set()\n",
        "\n",
        "for namefile in rilevazione_flusso:\n",
        "\n",
        "    year_dataset = namefile.split('_')[3].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        vehicle_count_set.update(vehicle_count_process_chunk(chunk))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(vehicle_count_set, '/content/vehicle_count_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "            # Reset the set\n",
        "            vehicle_count_set.clear()\n",
        "            piece += 1\n",
        "\n",
        "    save_graph(vehicle_count_set, '/content/vehicle_count_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "    vehicle_count_set.clear()\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gulLdqXsM_xm"
      },
      "outputs": [],
      "source": [
        "# Free memory\n",
        "del vehicle_count_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Pdf4EARn_p4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eaf7fe9-4d1a-4c79-9fdf-79584b0d550e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- populating vehicle accuracy ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 286974/286974 [06:24<00:00, 745.66it/s]\n",
            "100%|██████████| 292845/292845 [06:42<00:00, 727.53it/s]\n",
            "100%|██████████| 299130/299130 [06:53<00:00, 722.95it/s]\n",
            "100%|██████████| 316458/316458 [07:10<00:00, 734.61it/s]\n"
          ]
        }
      ],
      "source": [
        "print('--- populating vehicle accuracy ---')\n",
        "\n",
        "acc_set = set()\n",
        "\n",
        "for namefile in accuratezza_spire:\n",
        "\n",
        "    year_dataset = namefile.split('_')[2].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        acc_set.update(vehicle_accuracy_process_chunk(chunk))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(acc_set, '/content/vehicle_accuracy_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "            # Reset the set\n",
        "            acc_set.clear()\n",
        "            piece += 1\n",
        "\n",
        "    save_graph(acc_set, '/content/vehicle_accuracy_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "    acc_set.clear()\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MwKPMCZB_q41"
      },
      "outputs": [],
      "source": [
        "# Free memory\n",
        "del acc_set, coil_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uNf5npVF_rM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec22839-a2fb-4aeb-d789-9810a94c22f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- populating pollution data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76989/76989 [00:17<00:00, 4377.34it/s]\n",
            "100%|██████████| 77741/77741 [00:23<00:00, 3347.06it/s]\n",
            "100%|██████████| 79033/79033 [00:20<00:00, 3924.15it/s]\n",
            "100%|██████████| 79662/79662 [00:22<00:00, 3583.53it/s]\n"
          ]
        }
      ],
      "source": [
        "print('--- populating pollution data ---')\n",
        "\n",
        "pollution_set = set()\n",
        "\n",
        "for namefile in centraline:\n",
        "\n",
        "    year_dataset = namefile.split('_')[2].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        pollution_set.update(pollution_process_chunk(chunk))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(pollution_set, '/content/pollution_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "            # Reset the set\n",
        "            pollution_set.clear()\n",
        "            piece += 1\n",
        "\n",
        "    save_graph(pollution_set, '/content/pollution_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "    pollution_set.clear()\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OcS0-7v6AFWw"
      },
      "outputs": [],
      "source": [
        "# Free memory\n",
        "del pollution_set"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}