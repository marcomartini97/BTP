{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI9cUSUTuk6-",
        "outputId": "c2e28d1e-bbb4-46f5-8a8a-3dc7abf26b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.10/dist-packages (7.1.1)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from rdflib) (0.7.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install rdflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS8mKYNLvEeo",
        "outputId": "ddb05b56-a0a9-4106-f3af-c9089e9ca55c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datetime in /usr/local/lib/python3.10/dist-packages (5.5)\n",
            "Requirement already satisfied: zope.interface in /usr/local/lib/python3.10/dist-packages (from datetime) (7.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2024.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (75.1.0)\n"
          ]
        }
      ],
      "source": [
        "pip install datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6wKYePVvGrz",
        "outputId": "e86598a3-ed66-4dfe-dde6-40ea06ca7142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnbu2RlQvI3Z",
        "outputId": "46b8941e-325e-40e6-c294-81ac749a0436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"
          ]
        }
      ],
      "source": [
        "pip install psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "UdAPwvm7vKfU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "from rdflib import Graph, Literal, RDF, RDFS, URIRef, Namespace\n",
        "from rdflib.plugins.sparql import prepareQuery\n",
        "from rdflib.namespace import XSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "popuhA9jvOPN"
      },
      "outputs": [],
      "source": [
        "# To measure the usage of RAM\n",
        "import psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qohNwW5svO0d",
        "outputId": "8417da7e-a29a-44bd-f188-4b61f2f73752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Use your personal account!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "JsLnxkCOxZwY"
      },
      "outputs": [],
      "source": [
        "global pbar\n",
        "\n",
        "chunksize = 10000\n",
        "\n",
        "BTP = Namespace('http://www.dei.unipd.it/~gdb/ontology/btp/')\n",
        "\n",
        "global viaChiarini_gp, giardiniMargherita_gp, portaSanFelice_gp\n",
        "viaChiarini_gp = [44.4997732567231, 11.2873095406444]\n",
        "giardiniMargherita_gp = [44.4830615285162, 11.3528830371546] # via Medaro Bottonelli\n",
        "portaSanFelice_gp = [44.4991470592725, 11.3270506316853]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "-nvtR9CTvQ3q"
      },
      "outputs": [],
      "source": [
        "# Function to populate the coils dataset\n",
        "def coils_process_chunk(chunk : set) -> set:\n",
        "    # Graph\n",
        "    chunk_set = set()\n",
        "\n",
        "    # OWL - Object Properties\n",
        "    chunk_set.add('btp:isNearTo a owl:ObjectProperty .')\n",
        "    chunk_set.add('btp:isObserved a owl:ObjectProperty .')\n",
        "    chunk_set.add('btp:hasObserve a owl:ObjectProperty .')\n",
        "    chunk_set.add('btp:hasLevel a owl:ObjectProperty .')\n",
        "    chunk_set.add('btp:hasType a owl:ObjectProperty .')\n",
        "    chunk_set.add('btp:isOn a owl:ObjectProperty .')\n",
        "    chunk_set.add('btp:isPlaced a owl:ObjectProperty .')\n",
        "\n",
        "    # OWL - DataType Properties\n",
        "    chunk_set.add('btp:hasID a owl:DatatypeProperty .')\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        # I check if the record is valid or not -> must have all the field not NaN\n",
        "        if row['Livello'] == '' or row['tipologia'] == '' or row['codice arco'] == '':\n",
        "            # I skip the record -> next record\n",
        "            continue\n",
        "\n",
        "        # else: is valid -> continue\n",
        "\n",
        "        ## COIL:\n",
        "        # -uri: coil_ + id number.\n",
        "        # -attributi: hasID\n",
        "        # -object properties: hasLevel, hasType, isOn, and isPlaced.\n",
        "\n",
        "        Coil = 'btp:coil_'+str(row['ID_univoco_stazione_spira'])\n",
        "\n",
        "        # PollutionCoils and SimpleCoils are subclasses of Coil\n",
        "        chunk_set.add('btp:PollutionCoil rdfs:subClassOf btp:Coil .')\n",
        "        chunk_set.add('btp:SimpleCoil rdfs:subClassOf btp:Coil .')\n",
        "\n",
        "        # Cast to float\n",
        "        latitudine = row['latitudine']\n",
        "        longitudine = row['longitudine']\n",
        "\n",
        "        if(type(latitudine) == str):\n",
        "            latitudine = latitudine.replace(',', '')\n",
        "            # From 113473933293812,00 to 11.3473933293812\n",
        "            latitudine = latitudine[:2] + '.' + latitudine[2:]\n",
        "            # Cast to float\n",
        "            latitudine = float(latitudine)\n",
        "        if(type(longitudine) == str):\n",
        "            longitudine = longitudine.replace(',', '')\n",
        "            # From 44500438455000,00 to 44.500438455000\n",
        "            longitudine = longitudine[:2] + '.' + longitudine[2:]\n",
        "            longitudine = float(longitudine)\n",
        "\n",
        "        # Pollution coils -> must be around 300 m\n",
        "        if ((latitudine <= viaChiarini_gp[0] + 0.0027) and (latitudine >= viaChiarini_gp[0] + 0.0027)) and ((longitudine <= viaChiarini_gp[1] + 0.0013) and (longitudine >= viaChiarini_gp[1] - 0.0013)):\n",
        "            chunk_set.add(Coil + ' a btp:PollutionCoil .')\n",
        "            PollutionStation = 'btp:viaChiariniControlUnit'\n",
        "            chunk_set.add(PollutionStation + ' a btp:PollutionStation .')\n",
        "            chunk_set.add(PollutionStation + ' btp:isNearTo ' + Coil +' .')\n",
        "        elif ((latitudine <= giardiniMargherita_gp[0] + 0.0027) and (latitudine >= giardiniMargherita_gp[0] + 0.0027)) and ((longitudine <= giardiniMargherita_gp[1] + 0.0013) and (longitudine >= giardiniMargherita_gp[1] - 0.0013)):\n",
        "            chunk_set.add(Coil + ' a btp:PollutionCoil .')\n",
        "            PollutionStation = 'btp:giardiniMargheritaControlUnit'\n",
        "            chunk_set.add(PollutionStation + ' a btp:PollutionStation .')\n",
        "            chunk_set.add(PollutionStation + ' btp:isNearTo ' + Coil +' .')\n",
        "        elif ((latitudine <= portaSanFelice_gp[0] + 0.0027) and (latitudine >= portaSanFelice_gp[0] + 0.0027)) and ((longitudine <= portaSanFelice_gp[1] + 0.0013) and (longitudine >= portaSanFelice_gp[1] - 0.0013)):\n",
        "            chunk_set.add(Coil + ' a btp:PollutionCoil .')\n",
        "            PollutionStation = 'btp:portaSanFeliceControlUnit'\n",
        "            chunk_set.add(PollutionStation + ' a btp:PollutionStation .')\n",
        "            chunk_set.add(PollutionStation + ' btp:isNearTo ' + Coil +' .')\n",
        "        else:\n",
        "            chunk_set.add(Coil + ' a btp:SimpleCoil .')\n",
        "\n",
        "\n",
        "        for i in range(2, 26):\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "            VehicleDetection = 'btp:veDet_'+str(row['ID_univoco_stazione_spira'])+'_'+(date_obj.strftime('%Y-%m-%d')).replace('-', '_')+'_'+str(i-2).zfill(2)+'_'+str(i-1).zfill(2)\n",
        "            chunk_set.add(VehicleDetection + ' a btp:VehicleDetection .')\n",
        "            chunk_set.add(VehicleDetection + ' btp:isObserved ' + Coil + ' .')\n",
        "            chunk_set.add(Coil + ' btp:hasObserve ' + VehicleDetection + ' .')\n",
        "\n",
        "        Level = 'btp:level'+str(int(row['Livello']))\n",
        "        chunk_set.add(Level + ' a btp:Level .')\n",
        "        chunk_set.add(Coil + ' btp:hasLevel ' + Level + ' .')\n",
        "\n",
        "        Type = URIRef(BTP['type_'+str(row['tipologia'])])\n",
        "        Type = 'btp:'+str(row['tipologia'])\n",
        "        chunk_set.add(Type + ' a btp:Type .')\n",
        "        chunk_set.add(Coil + ' btp:hasType ' + Type + ' .')\n",
        "\n",
        "        chunk_set.add(Coil + ' btp:hasID \"' + str(row['codice spira']) + '\"^^xsd:string .')\n",
        "\n",
        "        RoadArch = 'btp:roadarch_'+str(row['codice arco'])\n",
        "        chunk_set.add(RoadArch + ' a btp:RoadArch .')\n",
        "        chunk_set.add(Coil + ' btp:isOn ' + RoadArch + ' .')\n",
        "        chunk_set.add(RoadArch + ' btp:isPlaced ' + Coil + ' .')\n",
        "\n",
        "    pbar.update(len(chunk))\n",
        "\n",
        "    return chunk_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "b4X79ZOAMSku"
      },
      "outputs": [],
      "source": [
        "# Function that populates the vehicle count dataset\n",
        "def vehicle_count_process_chunk(chunk: set) -> set:\n",
        "\n",
        "    vc_set = set()\n",
        "\n",
        "    # OWL - Object Properties\n",
        "    vc_set.add('btp:isObservedOnPeriod a owl:ObjectProperty .')\n",
        "    vc_set.add('btp:hasObservedOnPeriod a owl:ObjectProperty .')\n",
        "    vc_set.add('btp:onDay a owl:ObjectProperty .')\n",
        "\n",
        "    # OWL - DataType Properties\n",
        "    vc_set.add('btp:hasCount a owl:DatatypeProperty .')\n",
        "    vc_set.add('btp:startTime a owl:DatatypeProperty .')\n",
        "    vc_set.add('btp:endTime a owl:DatatypeProperty .')\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        # I check if the record is valid or not -> must have all the field not NaN\n",
        "        if row['Livello'] == '' or row['tipologia'] == '':\n",
        "            # I skip the record -> next record\n",
        "            continue\n",
        "        # else: is valid -> continue\n",
        "\n",
        "        for i in range(2, 26):\n",
        "\n",
        "            ## VEHICLEDETECTION:\n",
        "            # -uri: vehicleDetection_ + id number + _ + date.\n",
        "            # -attributi: hasCount.\n",
        "            # -object properties: isObserved, hasObserve, isObservedOnPeriod, and hasObservedOnPeriod.\n",
        "\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "            VehicleDetection = 'btp:veDet_'+str(row['ID_univoco_stazione_spira'])+'_'+(date_obj.strftime('%Y-%m-%d')).replace('-', '_')+'_'+str(i-2).zfill(2)+'_'+str(i-1).zfill(2)\n",
        "            vc_set.add(VehicleDetection + ' a btp:VehicleDetection .')\n",
        "\n",
        "            vc_set.add(VehicleDetection + ' btp:hasCount \"' + str(row.iloc[i]) + '\"^^xsd:integer .')\n",
        "\n",
        "            # # PERIOD:\n",
        "            # -uri: period_ + date + _ + hour1 + _ + hour2.\n",
        "            # -attributi: startTime and endTime.\n",
        "            # -object properties: onDay.\n",
        "\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "            Period = 'btp:period_'+(date_obj.strftime('%Y-%m-%d')).replace('-', '_')+'_'+str(i-2).zfill(2)+'_'+str(i-1).zfill(2)\n",
        "            vc_set.add(Period + ' a btp:Period .')\n",
        "\n",
        "            vc_set.add(Period + ' btp:isObservedOnPeriod ' + VehicleDetection + ' .')\n",
        "            vc_set.add(VehicleDetection + ' btp:hasObservedOnPeriod ' + Period + ' .')\n",
        "\n",
        "            startTime = str(i-2).zfill(2)+':00:00'\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "\n",
        "            vc_set.add(Period + ' btp:startTime \"' + str(date_obj.strftime('%Y-%m-%d')+'T'+startTime) + '\"^^xsd:dateTime .')\n",
        "\n",
        "            endTime = str(i-1).zfill(2)+':00:00'\n",
        "\n",
        "            # If the endTime is 24 -> date+1 and endTime = 00\n",
        "            if(endTime == '24:00:00'):\n",
        "                endTime = '00:00:00'\n",
        "                # I add one day\n",
        "                date_obj = date_obj + datetime.timedelta(days=1)\n",
        "\n",
        "            vc_set.add(Period + ' btp:endTime \"' + str(date_obj.strftime('%Y-%m-%d')+'T'+endTime) + '\"^^xsd:dateTime .')\n",
        "\n",
        "            ## Convert day from italian to english ex: lunedì -> monday\n",
        "            day_value = ''\n",
        "            if 'Giorno della settimana' in row:\n",
        "                day_value = str(row['Giorno della settimana']).lower()\n",
        "            elif 'giorno della settimana' in row:\n",
        "                day_value = str(row['giorno della settimana']).lower()\n",
        "\n",
        "            match day_value:\n",
        "                case 'lunedì':\n",
        "                    DayWeek = 'btp:monday'\n",
        "                    vc_set.add(DayWeek + ' a btp:DayWeek .')\n",
        "                    vc_set.add(Period + ' btp:onDay ' + DayWeek + ' .')\n",
        "                case 'martedì':\n",
        "                    DayWeek = 'btp:tuesday'\n",
        "                    vc_set.add(DayWeek + ' a btp:DayWeek .')\n",
        "                    vc_set.add(Period + ' btp:onDay ' + DayWeek + ' .')\n",
        "                case 'mercoledì':\n",
        "                    DayWeek = 'btp:wednesday'\n",
        "                    vc_set.add(DayWeek + ' a btp:DayWeek .')\n",
        "                    vc_set.add(Period + ' btp:onDay ' + DayWeek + ' .')\n",
        "                case 'giovedì':\n",
        "                    DayWeek = 'btp:thursday'\n",
        "                    vc_set.add(DayWeek + ' a btp:DayWeek .')\n",
        "                    vc_set.add(Period + ' btp:onDay ' + DayWeek + ' .')\n",
        "                case 'venerdì':\n",
        "                    DayWeek = 'btp:friday'\n",
        "                    vc_set.add(DayWeek + ' a btp:DayWeek .')\n",
        "                    vc_set.add(Period + ' btp:onDay ' + DayWeek + ' .')\n",
        "                case 'sabato':\n",
        "                    DayWeek = 'btp:saturday'\n",
        "                    vc_set.add(DayWeek + ' a btp:DayWeek .')\n",
        "                    vc_set.add(Period + ' btp:onDay ' + DayWeek + ' .')\n",
        "                case 'domenica':\n",
        "                    DayWeek = 'btp:sunday'\n",
        "                    vc_set.add(DayWeek + ' a btp:DayWeek .')\n",
        "                    vc_set.add(Period + ' btp:onDay ' + DayWeek + ' .')\n",
        "                case _:\n",
        "                    # No day provided\n",
        "                    pass\n",
        "\n",
        "    pbar.update(len(chunk))\n",
        "\n",
        "    return vc_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "qNfIBNPI_apI"
      },
      "outputs": [],
      "source": [
        "# Function that populates the vehicle accuracy dataset\n",
        "def vehicle_accuracy_process_chunk(chunk : set, coil_graph : Graph) -> set:\n",
        "\n",
        "    # Graphs\n",
        "    acc_set = set()\n",
        "\n",
        "    # OWL - DataType Properties\n",
        "    acc_set.add('btp:hasAccuracy a owl:DatatypeProperty .')\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        for i in range(2, 26):\n",
        "\n",
        "            ## VEHICLEDETECTION:\n",
        "            # -uri: vehicleDetection_ + id number + _ + date.\n",
        "            # -attributi: hasAccuracy, and hasCount.\n",
        "\n",
        "            coil = ''\n",
        "\n",
        "            # Query to get the coil's code associated to an ID\n",
        "            coil = get_coil_by_id(str(row['codice spira']), coil_graph)\n",
        "            if coil == '':\n",
        "                # I skip the record -> next record\n",
        "                continue\n",
        "\n",
        "            date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "\n",
        "            VehicleDetection = 'btp:veDet_' + str(coil) + '_' + (str(date_obj.strftime('%Y-%m-%d'))).replace('-', '_') + '_'+str(i-2).zfill(2) + '_' + str(i-1).zfill(2)\n",
        "            acc_set.add(VehicleDetection + ' a btp:VehicleDetection .')\n",
        "            percentage = row.iloc[i].replace('%', '')\n",
        "            acc_set.add(VehicleDetection + ' btp:hasAccuracy \"' + str(float(percentage)) + '\"^^xsd:float .')\n",
        "\n",
        "    pbar.update(len(chunk))\n",
        "\n",
        "    return acc_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "BG0Zbtji_Wan"
      },
      "outputs": [],
      "source": [
        "# Function that populates the pollution data\n",
        "def pollution_process_chunk(chunk: set) -> set:\n",
        "\n",
        "    pol_set = set()\n",
        "\n",
        "    # OWL - Object Properties\n",
        "    pol_set.add('btp:hasDetect a owl:ObjectProperty .')\n",
        "    pol_set.add('btp:isDetected a owl:ObjectProperty .')\n",
        "    pol_set.add('btp:isRegistered a owl:ObjectProperty .')\n",
        "    pol_set.add('btp:hasRegister a owl:ObjectProperty .')\n",
        "    pol_set.add('btp:isDetectedOnPeriod a owl:ObjectProperty .')\n",
        "    pol_set.add('btp:hasDetectedOnPeriod a owl:ObjectProperty .')\n",
        "\n",
        "    # OWL - DateType Properties\n",
        "    pol_set.add('btp:startTime a owl:DatatypeProperty .')\n",
        "    pol_set.add('btp:endTime a owl:DatatypeProperty .')\n",
        "    pol_set.add('btp:inQuantity a owl:DatatypeProperty .')\n",
        "    pol_set.add('btp:hasChemicalName a owl:DatatypeProperty .')\n",
        "\n",
        "    for index, row in chunk.iterrows():\n",
        "\n",
        "        ## POLLUTIONSTATION:\n",
        "        # -uri: centralUnit + pollution name.\n",
        "        # -object properties: hasRegister, and isRegistered.\n",
        "\n",
        "        PollutionStation = 'btp:' + ((str(row['COD_STAZ']).split(\" \"))[0]).lower() + ''.join(s.capitalize() for s in (str(row['COD_STAZ']).split(\" \"))[1:]) + 'ControlUnit'\n",
        "        pol_set.add(PollutionStation + ' a btp:PollutionStation .')\n",
        "\n",
        "        # PERIOD:\n",
        "        # -uri: period_ + date + _ + hour1 + _ + hour2.\n",
        "        # -attributi: startTime and endTime.\n",
        "        # -object properties: onDay.\n",
        "\n",
        "        # date format: yyyy-mm-ddThh:mm:ss+hh:mm\n",
        "        # keep only the data: 'Thh:mm:ss+hh:mm' -> yyyy-mm-dd\n",
        "        date_obj = datetime.datetime.strptime((str(row['DATA_INIZIO']).split('T'))[0], '%Y-%m-%d')\n",
        "        # keep only the hour: 'Thh:mm:ss+hh:mm' -> hh:mm:ss\n",
        "        startTime = str((((str(row['DATA_INIZIO']).split('T'))[1].split('+')[0]).split(':'))[0])\n",
        "        endTime = str((((str(row['DATA_FINE']).split('T'))[1].split('+')[0]).split(':'))[0])\n",
        "\n",
        "        Period = 'btp:period_'+(str(date_obj.strftime('%Y-%m-%d'))).replace('-', '_')+'_'+startTime+'_'+endTime\n",
        "        pol_set.add(Period + ' a btp:Period .')\n",
        "\n",
        "        chemical_element = ((row['AGENTE'].split('(')[0]).strip()).upper()\n",
        "        date_obj = datetime.datetime.strptime((str(row['DATA_INIZIO']).split('T'))[0], '%Y-%m-%d')\n",
        "\n",
        "        ## CHEMICALDETECTION:\n",
        "        # -uri: chemicalDetection_ + pollution_station_name + _ + date + _ + element.\n",
        "        # -attributi: inQuantity (conversion all in ug/m), and hasChemicalName.\n",
        "        # -object properties: isDetectedOnPeriod, hasDetectedOnPeriod, hasDetect, and isDetected.\n",
        "\n",
        "        ChemicalDetection = 'btp:chDet_'+(str(row['COD_STAZ']).lower()).replace(' ', '')+'_'+(str(date_obj.strftime('%Y-%m-%d'))).replace('-', '_')+'_'+startTime+'_'+endTime+'_'+chemical_element\n",
        "        pol_set.add(ChemicalDetection + ' a btp:ChemicalDetection .')\n",
        "        pol_set.add(ChemicalDetection + ' btp:isRegistered ' + PollutionStation + ' .')\n",
        "        pol_set.add(PollutionStation + ' btp:hasRegister ' + ChemicalDetection + ' .')\n",
        "\n",
        "        pol_set.add(Period + ' btp:isDetectedOnPeriod ' + ChemicalDetection + ' .')\n",
        "        pol_set.add(ChemicalDetection + ' btp:hasDetectedOnPeriod ' + Period + ' .')\n",
        "\n",
        "        # Cast from mg/m^3 to ug/m^3\n",
        "        if(row['UM'] == 'mg/m3'):\n",
        "            pol_set.add(ChemicalDetection + ' btp:inQuantity \"' + str(float(row['VALORE']*1000)) + '\"^^xsd:float .')\n",
        "        else:\n",
        "            pol_set.add(ChemicalDetection + ' btp:inQuantity \"' + str(float(row['VALORE'])) + '\"^^xsd:float .')\n",
        "\n",
        "        ## CHEMICALELEMENT:\n",
        "        # -uri: chemicalElement_ + chemical element name.\n",
        "        # -object properties: hasDetect, and isDetected\n",
        "\n",
        "        ChemicalElement = 'btp:'+chemical_element\n",
        "        pol_set.add(ChemicalElement + ' a btp:ChemicalElement .')\n",
        "        pol_set.add(ChemicalDetection + ' btp:hasDetect ' + ChemicalElement + ' .')\n",
        "        pol_set.add(ChemicalElement + ' btp:isDetected ' + ChemicalDetection + ' .')\n",
        "\n",
        "        if len(row['AGENTE'].split('(')) > 1:\n",
        "            chemical_element_name = (((row['AGENTE'].split('(')[1]).replace(')','')).strip()).lower()\n",
        "\n",
        "            match chemical_element_name:\n",
        "                case 'benzene':\n",
        "                    pol_set.add(ChemicalDetection + ' btp:hasChemicalName \"Benzene\"^^xsd:string .')\n",
        "                case 'monossido di carbonio':\n",
        "                    pol_set.add(ChemicalDetection + ' btp:hasChemicalName \"Carbon monoxide\"^^xsd:string .')\n",
        "                case 'monossido di azoto':\n",
        "                    pol_set.add(ChemicalDetection + ' btp:hasChemicalName \"Nitrogen Monoxide\"^^xsd:string .')\n",
        "                case 'biossido di azoto':\n",
        "                    pol_set.add(ChemicalDetection + ' btp:hasChemicalName \"Nitrogen dioxide\"^^xsd:string .')\n",
        "                case 'ossidi di azoto':\n",
        "                    pol_set.add(ChemicalDetection + ' btp:hasChemicalName \"Nitrogen oxides\"^^xsd:string .')\n",
        "                case 'ozono':\n",
        "                    pol_set.add(ChemicalDetection + ' btp:hasChemicalName \"Ozone\"^^xsd:string .')\n",
        "                case _:\n",
        "                    # New element provided\n",
        "                    pol_set.add(ChemicalDetection + ' btp:hasChemicalName \"' + chemical_element_name + '\"^^xsd:string .')\n",
        "\n",
        "        startTime = startTime+':00:00'\n",
        "        endTime = endTime+':00:00'\n",
        "\n",
        "        pol_set.add(Period + ' btp:startTime \"' + str(date_obj.strftime('%Y-%m-%d')+'T'+startTime) + '\"^^xsd:dateTime .')\n",
        "        pol_set.add(Period + ' btp:endTime \"' + str(date_obj.strftime('%Y-%m-%d')+'T'+endTime) + '\"^^xsd:dateTime .')\n",
        "\n",
        "    pbar.update(len(chunk))\n",
        "\n",
        "    return pol_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "yKKpAEOJvkXc"
      },
      "outputs": [],
      "source": [
        "# Function to save a graph\n",
        "def save_graph(set : set, path : str):\n",
        "\n",
        "    with open(path, 'w') as file:\n",
        "\n",
        "        file.write('@prefix btp: <' + BTP + '> .\\n')\n",
        "        file.write('@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\\n')\n",
        "        file.write('@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\\n')\n",
        "        file.write('@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n')\n",
        "        file.write('@prefix owl: <http://www.w3.org/2002/07/owl#> .\\n')\n",
        "\n",
        "        file.write('\\n')\n",
        "\n",
        "        for elem in set:\n",
        "            file.write(elem + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "2AKBbNI1_cFq"
      },
      "outputs": [],
      "source": [
        "def get_coil_by_id(coil_id : str, graph : Graph) -> str:\n",
        "\n",
        "    code_coil_query = prepareQuery(\"\"\"\n",
        "    SELECT DISTINCT ?coil WHERE {\n",
        "        ?coil btp:hasID ?id .\n",
        "    FILTER (?id = ?coil_id)\n",
        "                               }\"\"\" , initNs={'btp': BTP})\n",
        "\n",
        "    res = graph.query(code_coil_query, initBindings={'coil_id':Literal(coil_id, datatype=XSD.string)})\n",
        "    if res == [] or res == None:\n",
        "        return ''\n",
        "    else:\n",
        "        for r in res:\n",
        "            return str(r.coil).replace('http://www.dei.unipd.it/~gdb/ontology/btp/coil_', '')\n",
        "            # To be sure that is only one element\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "cc3JSytzvaTk"
      },
      "outputs": [],
      "source": [
        "## Datasets\n",
        "\n",
        "# Rilevazione flusso datasets\n",
        "rilevazione_flusso = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/rilevazione_flusso_veicoli_2019.csv')\n",
        "\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2019.csv')\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2020.csv')\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2021.csv')\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2022.csv')\n",
        "\n",
        "# Accuratezza spire datasets\n",
        "accuratezza_spire = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/accuratezza_spire_2019.csv')\n",
        "\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2019.csv')\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2020.csv')\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2021.csv')\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2022.csv')\n",
        "\n",
        "# Centraline qualità datasets\n",
        "centraline = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/dati_centraline_2019.csv')\n",
        "\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2019.csv')\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2020.csv')\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2021.csv')\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2022.csv')\n",
        "\n",
        "# Save path\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/Graph Database/rdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "dYvPAPAzvglq"
      },
      "outputs": [],
      "source": [
        "# I check if the folder is empty or not\n",
        "if not os.listdir(save_path) == []:\n",
        "    print('The folder is not empty, do you want to continue? (y/n)')\n",
        "    answer = input()\n",
        "    if(answer.lower() == 'y'):\n",
        "        # I remove all the files in the folder\n",
        "        print('Removing all the files in the folder ...')\n",
        "        for file in os.listdir(save_path):\n",
        "            os.remove(os.path.join(save_path, file))\n",
        "        print('DONE!')\n",
        "    else:\n",
        "        exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8xpGnlBv019",
        "outputId": "bed1bfa5-6885-4914-9e93-88269b0c83f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- populating coils ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:00<00:00, 737.54it/s]\n"
          ]
        }
      ],
      "source": [
        "print('--- populating coils ---')\n",
        "\n",
        "coils_set = set()\n",
        "coils_subset = set()\n",
        "\n",
        "# Regular expression\n",
        "re_a_type_pol = re.compile(r' a btp:PollutionStation .')\n",
        "re_a_type_sm = re.compile(r' a btp:SimpleCoil .')\n",
        "re_hasID = re.compile(r' btp:hasID ')\n",
        "\n",
        "for namefile in rilevazione_flusso:\n",
        "\n",
        "    year_dataset = namefile.split('_')[3].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        coils_set.update(coils_process_chunk(chunk))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(coils_set, '/content/coils_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "            # Reset the set\n",
        "            coils_set.clear()\n",
        "            piece += 1\n",
        "\n",
        "    save_graph(coils_set, '/content/coils_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "\n",
        "    # Subset for coils - hasID\n",
        "    coils_subset.update({s for s in coils_set if re_hasID.search(str(s))})\n",
        "\n",
        "    # Subset for coils - a btp:PollutionStation\n",
        "    coils_subset.update({s for s in coils_set if re_a_type_pol.search(str(s))})\n",
        "\n",
        "    # Subset for coils - a btp:SimpleCoil\n",
        "    coils_subset.update({s for s in coils_set if re_a_type_sm.search(str(s))})\n",
        "\n",
        "\n",
        "    coils_set.clear()\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "coils_subset.add('btp:PollutionCoil rdfs:subClassOf btp:Coil .')\n",
        "coils_subset.add('btp:SimpleCoil rdfs:subClassOf btp:Coil .')\n",
        "\n",
        "# Save the subset\n",
        "save_graph(coils_subset, '/content/coils_subset_populated.ttl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "As2AF1eCwUrG"
      },
      "outputs": [],
      "source": [
        "# Free memory\n",
        "del coils_set, coils_subset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEjxAUd2Muhc",
        "outputId": "3bf884d8-a9e6-42aa-c2cc-b0799dd0b6a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- populating coils ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:00<00:00, 354.54it/s]\n"
          ]
        }
      ],
      "source": [
        "print('--- populating coils ---')\n",
        "\n",
        "vehicle_count_set = set()\n",
        "\n",
        "for namefile in rilevazione_flusso:\n",
        "\n",
        "    year_dataset = namefile.split('_')[3].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        vehicle_count_set.update(vehicle_count_process_chunk(chunk))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(vehicle_count_set, '/content/vehicle_count_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "            # Reset the set\n",
        "            vehicle_count_set.clear()\n",
        "            piece += 1\n",
        "\n",
        "    save_graph(vehicle_count_set, '/content/vehicle_count_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "    vehicle_count_set.clear()\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "gulLdqXsM_xm"
      },
      "outputs": [],
      "source": [
        "# Free memory\n",
        "del vehicle_count_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pdf4EARn_p4U",
        "outputId": "0e41d500-2aac-434b-e8d1-360e6aec1eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- populating vehicle accuracy ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:21<00:00,  1.16it/s]\n"
          ]
        }
      ],
      "source": [
        "print('--- populating vehicle accuracy ---')\n",
        "\n",
        "acc_set = set()\n",
        "\n",
        "coils_graph = Graph()\n",
        "coils_graph.parse('/content/coils_subset_populated.ttl', format='ttl')\n",
        "\n",
        "for namefile in accuratezza_spire:\n",
        "\n",
        "    year_dataset = namefile.split('_')[2].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        acc_set.update(vehicle_accuracy_process_chunk(chunk, coils_graph))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(acc_set, '/content/vehicle_accuracy_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "            # Reset the set\n",
        "            acc_set.clear()\n",
        "            piece += 1\n",
        "\n",
        "    save_graph(acc_set, '/content/vehicle_accuracy_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "    acc_set.clear()\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "MwKPMCZB_q41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "9b0bcfc1-7f0c-4b20-9a96-70f4aa04b2fa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'acc_set' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-2a33a9fb2c84>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Free memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0macc_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'acc_set' is not defined"
          ]
        }
      ],
      "source": [
        "# Free memory\n",
        "del acc_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNf5npVF_rM8",
        "outputId": "d018056a-7515-4023-bd93-631d483e9205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- populating pollution data ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 182/182 [00:00<00:00, 4399.46it/s]\n"
          ]
        }
      ],
      "source": [
        "print('--- populating pollution data ---')\n",
        "\n",
        "pollution_set = set()\n",
        "\n",
        "for namefile in centraline:\n",
        "\n",
        "    year_dataset = namefile.split('_')[2].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Manage NaN values\n",
        "        chunk = chunk.fillna('')\n",
        "\n",
        "        # Add the coils to the set\n",
        "        pollution_set.update(pollution_process_chunk(chunk))\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 85:\n",
        "            save_graph(pollution_set, '/content/pollution_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "            # Reset the set\n",
        "            pollution_set.clear()\n",
        "            piece += 1\n",
        "\n",
        "    save_graph(pollution_set, '/content/pollution_populated_'+year_dataset+'_'+str(piece)+'.ttl')\n",
        "    pollution_set.clear()\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "OcS0-7v6AFWw"
      },
      "outputs": [],
      "source": [
        "# Free memory\n",
        "del pollution_set"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}