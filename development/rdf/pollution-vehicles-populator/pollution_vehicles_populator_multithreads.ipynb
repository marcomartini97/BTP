{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5Ilx5vB_lj-"
      },
      "source": [
        "**Pollution Vehicles Populator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIBupZyT_dpj"
      },
      "outputs": [],
      "source": [
        "pip install rdflib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb86ZFPHAO07"
      },
      "outputs": [],
      "source": [
        "pip install datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyuiYQzZASPX"
      },
      "outputs": [],
      "source": [
        "pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imRHuIOSQsC8"
      },
      "outputs": [],
      "source": [
        "pip install psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z632iBgvH6H3"
      },
      "outputs": [],
      "source": [
        "pip install thread6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzMpCUFg_5n_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import re\n",
        "\n",
        "from rdflib import Graph, Literal, RDF, RDFS, URIRef, Namespace\n",
        "from rdflib.plugins.sparql import prepareQuery\n",
        "from rdflib.namespace import XSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0vjYh4zQlEn"
      },
      "outputs": [],
      "source": [
        "# To measure the usage of RAM\n",
        "import psutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mNp04jeIAbA"
      },
      "outputs": [],
      "source": [
        "# For multithreading\n",
        "import threading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3F_GyAOAGXZ"
      },
      "outputs": [],
      "source": [
        "# Use your personal account!\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHJR6MpGIF4S"
      },
      "outputs": [],
      "source": [
        "# Function to save a graph - multi-threading\n",
        "def save_graph(graph, path):\n",
        "    with open(path, 'w') as file:\n",
        "        file.write(graph.serialize(format='turtle'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Variables\n",
        "\n",
        "# List of open threads\n",
        "threads = []\n",
        "print(f'Number of threads: {psutil.cpu_count()}')\n",
        "print(f'Number of core: {psutil.cpu_count(logical=False)}')"
      ],
      "metadata": {
        "id": "FQmrNqP6gNSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfAOhJgX_8B1"
      },
      "outputs": [],
      "source": [
        "## Datasets\n",
        "\n",
        "# Rilevazione flusso datasets\n",
        "rilevazione_flusso = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/rilevazione_flusso_veicoli_2019.csv')\n",
        "\n",
        "rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2019.csv')\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2020.csv')\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2021.csv')\n",
        "# rilevazione_flusso.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/rilevazione_flusso_veicoli_2022.csv')\n",
        "\n",
        "# Accuratezza spire datasets\n",
        "accuratezza_spire = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/accuratezza_spire_2019.csv')\n",
        "\n",
        "accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2019.csv')\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2020.csv')\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2021.csv')\n",
        "# accuratezza_spire.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/accuratezza_spire_2022.csv')\n",
        "\n",
        "# Centraline qualitÃ  datasets\n",
        "centraline = []\n",
        "\n",
        "# ONLY FOR TEST\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/test/dati_centraline_2019.csv')\n",
        "\n",
        "centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2019.csv')\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2020.csv')\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2021.csv')\n",
        "# centraline.append('/content/drive/MyDrive/Colab Notebooks/Graph Database/datasets/dati_centraline_2022.csv')\n",
        "\n",
        "# Save path\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/Graph Database/rdf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QNzoF7LBSSs"
      },
      "outputs": [],
      "source": [
        "# Chunksize (aviod memory error)\n",
        "chunksize = 100\n",
        "\n",
        "# Define the Namespace\n",
        "BTP = Namespace(\"http://www.dei.unipd.it/~gdb/ontology/btp/#\")\n",
        "\n",
        "# Pollution coils geopoint -> from google maps!\n",
        "viaChiarini_gp = [44.4997732567231, 11.2873095406444]\n",
        "giardiniMargherita_gp = [44.4830615285162, 11.3528830371546] # via Medaro Bottonelli\n",
        "portaSanFelice_gp = [44.4991470592725, 11.3270506316853]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bsn1PtgBthRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ31zzZCBZOs"
      },
      "outputs": [],
      "source": [
        "# I check if the folder is empty or not\n",
        "if not os.listdir(save_path) == []:\n",
        "    print(\"The folder is not empty, do you want to continue? (y/n)\")\n",
        "    answer = input()\n",
        "    if(answer.lower() == 'y'):\n",
        "        # I remove all the files in the folder\n",
        "        print(\"Removing all the files in the folder ...\")\n",
        "        for file in os.listdir(save_path):\n",
        "            os.remove(os.path.join(save_path, file))\n",
        "        print(\"DONE!\")\n",
        "    else:\n",
        "        exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcKaEc7rveTJ"
      },
      "outputs": [],
      "source": [
        "print(\"--- populating coils ---\")\n",
        "\n",
        "for namefile in rilevazione_flusso:\n",
        "\n",
        "    year_dataset = namefile.split('_')[3].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Graph\n",
        "        g_coils = Graph()\n",
        "\n",
        "        # Bind Namespaces\n",
        "        g_coils.bind(\"xsd\", XSD)\n",
        "        g_coils.bind(\"btp\", BTP)\n",
        "\n",
        "        for index, row in chunk.iterrows():\n",
        "\n",
        "            # I check if the record is valid or not -> must have all the field not NaN\n",
        "            if row['Livello'] == '' and row['tipologia'] == '' and row['Nome via'] == '':\n",
        "                # I skip the record -> next record\n",
        "                continue\n",
        "            # else: is valid -> continue\n",
        "\n",
        "            ## COIL:\n",
        "            # -uri: coil_ + id number.\n",
        "            # -attributi: hasID\n",
        "            # -object properties: hasLevel, hasType, isOn, and isPlacedOn.\n",
        "\n",
        "            Coil = URIRef(BTP[\"coil_\"+str(row['ID_univoco_stazione_spira'])])\n",
        "\n",
        "            # PollutionCoils and SimpleCoils are subclasses of Coil\n",
        "            g_coils.add((BTP.SimpleCoil, RDFS.subClassOf, BTP.Coil))\n",
        "            g_coils.add((BTP.PollutionCoil, RDFS.subClassOf, BTP.Coil))\n",
        "\n",
        "            # Cast to float\n",
        "            latitudine = row['latitudine']\n",
        "            longitudine = row['longitudine']\n",
        "\n",
        "            if(type(latitudine) == str):\n",
        "                latitudine = latitudine.replace(',', '')\n",
        "                # From 113473933293812,00 to 11.3473933293812\n",
        "                latitudine = latitudine[:2] + '.' + latitudine[2:]\n",
        "                # Cast to float\n",
        "                latitudine = float(latitudine)\n",
        "            if(type(longitudine) == str):\n",
        "                longitudine = longitudine.replace(',', '')\n",
        "                # From 44500438455000,00 to 44.500438455000\n",
        "                longitudine = longitudine[:2] + '.' + longitudine[2:]\n",
        "                longitudine = float(longitudine)\n",
        "\n",
        "            # Pollution coils -> must be around 300 m\n",
        "            if ((latitudine <= viaChiarini_gp[0] + 0.0027) and (latitudine >= viaChiarini_gp[0] + 0.0027)) and ((longitudine <= viaChiarini_gp[1] + 0.0013) and (longitudine >= viaChiarini_gp[1] - 0.0013)):\n",
        "                g_coils.add((Coil, RDF.type, BTP.PollutionCoil))\n",
        "                PollutionStation = URIRef(BTP[\"controlUnitViaChiarini\"])\n",
        "                g_coils.add((PollutionStation, RDF.type, BTP.PollutionStation))\n",
        "                g_coils.add((PollutionStation, BTP.isNearTo, Coil))\n",
        "            elif ((latitudine <= giardiniMargherita_gp[0] + 0.0027) and (latitudine >= giardiniMargherita_gp[0] + 0.0027)) and ((longitudine <= giardiniMargherita_gp[1] + 0.0013) and (longitudine >= giardiniMargherita_gp[1] - 0.0013)):\n",
        "                g_coils.add((Coil, RDF.type, BTP.PollutionCoil))\n",
        "                PollutionStation = URIRef(BTP[\"controlUnitGiardiniMargherita\"])\n",
        "                g_coils.add((PollutionStation, RDF.type, BTP.PollutionStation))\n",
        "                g_coils.add((PollutionStation, BTP.isNearTo, Coil))\n",
        "            elif ((latitudine <= portaSanFelice_gp[0] + 0.0027) and (latitudine >= portaSanFelice_gp[0] + 0.0027)) and ((longitudine <= portaSanFelice_gp[1] + 0.0013) and (longitudine >= portaSanFelice_gp[1] - 0.0013)):\n",
        "                g_coils.add((Coil, RDF.type, BTP.PollutionCoil))\n",
        "                PollutionStation = URIRef(BTP[\"controlUnitPortaSanFelice\"])\n",
        "                g_coils.add((PollutionStation, RDF.type, BTP.PollutionStation))\n",
        "                g_coils.add((PollutionStation, BTP.isNearTo, Coil))\n",
        "            else:\n",
        "                g_coils.add((Coil, RDF.type, BTP.SimpleCoil))\n",
        "\n",
        "\n",
        "            for i in range(2, 26):\n",
        "                date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "                VehicleDetection = URIRef(BTP[\"vehicleDetection_\"+str(row['ID_univoco_stazione_spira'])+\"_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+str(i-2).zfill(2)+\":00-\"+str(i-1).zfill(2)+\":00\"])\n",
        "                g_coils.add((VehicleDetection, RDF.type, BTP.VehicleDetection))\n",
        "\n",
        "                g_coils.add((VehicleDetection, BTP.isObserved, Coil))\n",
        "                g_coils.add((Coil, BTP.hasObserve, VehicleDetection))\n",
        "\n",
        "            ################################################################\n",
        "\n",
        "            Level = URIRef(BTP[\"level_\"+str(row['Livello'])])\n",
        "            g_coils.add((Level, RDF.type, BTP.Level))\n",
        "            g_coils.add((Coil, BTP.hasLevel, Level))\n",
        "\n",
        "            Type = URIRef(BTP[\"type_\"+str(row['tipologia'])])\n",
        "            g_coils.add((Type, RDF.type, BTP.Type))\n",
        "            g_coils.add((Coil, BTP.hasType, Type))\n",
        "\n",
        "            g_coils.add((Coil, BTP.hasID, Literal(str(row['codice spira']), datatype=XSD.string)))\n",
        "\n",
        "            # Add the road\n",
        "            if(row['codice via'] == ''):\n",
        "                continue\n",
        "                # MARCO's CODE\n",
        "\n",
        "            # Road here can't be empty\n",
        "            Road = URIRef(BTP[\"road_\"+str(row['codice via'])])\n",
        "            g_coils.add((Coil, BTP.isOn, Road))\n",
        "            g_coils.add((Road, BTP.isPlacedOn, Coil))\n",
        "            g_coils.add((Road, RDFS.label, Literal(str(row['Nome via']).lower(), datatype=XSD.string)))\n",
        "\n",
        "        pbar.update(chunksize)\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 70 or len(threads) == psutil.cpu_count():\n",
        "            # I'm using more than 70% of the whole RAM or I'm using all the threads\n",
        "            for thread in threads:\n",
        "                thread.join()\n",
        "            threads = []\n",
        "\n",
        "        # Save the graph in a new thread\n",
        "        thread = threading.Thread(target=save_graph, args=(g_coils, '/content/coils_populated_'+year_dataset+'_'+str(piece)+'.ttl'))\n",
        "        thread.start()\n",
        "        threads.append(thread)\n",
        "        # Ready for the next piece\n",
        "        piece += 1\n",
        "\n",
        "        # Free memory\n",
        "        del g_coils\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVhdqo2eIZ1j"
      },
      "outputs": [],
      "source": [
        "# Wait for all the threads to finish\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "print(\"--- merging coils ---\")\n",
        "\n",
        "merged_graph = Graph()\n",
        "\n",
        "regex = re.compile(r'coils_populated_\\d{4}_\\d+\\.ttl')\n",
        "\n",
        "files = []\n",
        "\n",
        "for filename in os.listdir('/content/'):\n",
        "    if regex.match(filename):\n",
        "        files.append(filename)\n",
        "\n",
        "pbar = tqdm(total=len(files))\n",
        "\n",
        "for filename in files:\n",
        "    merged_graph.parse(os.path.join('/content/', filename), format='turtle')\n",
        "    pbar.update(1)\n",
        "\n",
        "# Save the merged graph\n",
        "merged_graph.serialize(destination=os.path.join(save_path, 'coils_populated.ttl'), format='turtle')\n",
        "\n",
        "# Remove all the partial files in the directory\n",
        "for filename in os.listdir(save_path):\n",
        "    if regex.match(filename):\n",
        "        os.remove(os.path.join(save_path, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEwPrakqBeAA"
      },
      "outputs": [],
      "source": [
        "print(\"--- populating vehicle count ---\")\n",
        "\n",
        "for namefile in rilevazione_flusso:\n",
        "\n",
        "    year_dataset = namefile.split('_')[3].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Graph\n",
        "        g_vc = Graph()\n",
        "\n",
        "        # Bind Namespaces\n",
        "        g_vc.bind(\"xsd\", XSD)\n",
        "        g_vc.bind(\"btp\", BTP)\n",
        "\n",
        "        for index, row in chunk.iterrows():\n",
        "\n",
        "            # I check if the record is valid or not -> must have all the field not NaN\n",
        "            if row['Livello'] == '' and row['tipologia'] == '' and row['Nome via'] == '':\n",
        "                # I skip the record -> next record\n",
        "                continue\n",
        "            # else: is valid -> continue\n",
        "\n",
        "            for i in range(2, 26):\n",
        "\n",
        "                ## VEHICLEDETECTION:\n",
        "                # -uri: vehicleDetection_ + id number + _ + date.\n",
        "                # -attributi: hasCount.\n",
        "                # -object properties: isObserved, hasObserve, isObservedOnPeriod, and hasObservedOnPeriod.\n",
        "\n",
        "                date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "                VehicleDetection = URIRef(BTP[\"vehicleDetection_\"+str(row['ID_univoco_stazione_spira'])+\"_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+str(i-2).zfill(2)+\":00-\"+str(i-1).zfill(2)+\":00\"])\n",
        "                g_vc.add((VehicleDetection, RDF.type, BTP.VehicleDetection))\n",
        "\n",
        "                g_vc.add((VehicleDetection, BTP.hasCount, Literal(row.iloc[i], datatype=XSD.integer)))\n",
        "\n",
        "                # # PERIOD:\n",
        "                # -uri: period_ + date + _ + hour1 + _ + hour2.\n",
        "                # -attributi: startTime and endTime.\n",
        "                # -object properties: onDay.\n",
        "\n",
        "                date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "                Period = URIRef(BTP[\"period_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+str(i-2).zfill(2)+\":00-\"+str(i-1).zfill(2)+\":00\"])\n",
        "                g_vc.add((Period, RDF.type, BTP.Period))\n",
        "\n",
        "                g_vc.add((Period, BTP.isObservedOnPeriod, VehicleDetection))\n",
        "                g_vc.add((VehicleDetection, BTP.hasObservedOnPeriod, Period))\n",
        "\n",
        "                startTime = str(i-2).zfill(2)+\":00\"\n",
        "                date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "\n",
        "                g_vc.add((Period, BTP.startTime, Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+startTime, datatype=XSD.dateTime)))\n",
        "\n",
        "                endTime = str(i-1).zfill(2)+\":00\"\n",
        "\n",
        "                # If the endTime is 24 -> date+1 and endTime = 00\n",
        "                if(endTime == '24:00'):\n",
        "                    endTime = '00:00'\n",
        "                    # I add one day\n",
        "                    date_obj = date_obj + datetime.timedelta(days=1)\n",
        "\n",
        "                g_vc.add((Period, BTP.endTime, Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+endTime, datatype=XSD.dateTime)))\n",
        "\n",
        "                ## Convert day from italian to english ex: lunedÃ¬ -> monday\n",
        "                day_value = ''\n",
        "                if 'Giorno della settimana' in row:\n",
        "                    day_value = str(row['Giorno della settimana']).lower()\n",
        "                elif 'giorno della settimana' in row:\n",
        "                    day_value = str(row['giorno della settimana']).lower()\n",
        "\n",
        "                match day_value:\n",
        "                    case 'lunedÃ¬':\n",
        "                        DayWeek = URIRef(BTP[\"Monday\"])\n",
        "                        g_vc.add((DayWeek, RDF.type, BTP.DayWeek))\n",
        "                        g_vc.add((Period, BTP.onDay, DayWeek))\n",
        "                    case 'martedÃ¬':\n",
        "                        DayWeek = URIRef(BTP[\"Tuesday\"])\n",
        "                        g_vc.add((DayWeek, RDF.type, BTP.DayWeek))\n",
        "                        g_vc.add((Period, BTP.onDay, DayWeek))\n",
        "                    case 'mercoledÃ¬':\n",
        "                        DayWeek = URIRef(BTP[\"Wednesday\"])\n",
        "                        g_vc.add((DayWeek, RDF.type, BTP.DayWeek))\n",
        "                        g_vc.add((Period, BTP.onDay, DayWeek))\n",
        "                    case 'giovedÃ¬':\n",
        "                        DayWeek = URIRef(BTP[\"Thursday\"])\n",
        "                        g_vc.add((DayWeek, RDF.type, BTP.DayWeek))\n",
        "                        g_vc.add((Period, BTP.onDay, DayWeek))\n",
        "                    case 'venerdÃ¬':\n",
        "                        DayWeek = URIRef(BTP[\"Friday\"])\n",
        "                        g_vc.add((DayWeek, RDF.type, BTP.DayWeek))\n",
        "                        g_vc.add((Period, BTP.onDay, DayWeek))\n",
        "                    case 'sabato':\n",
        "                        DayWeek = URIRef(BTP[\"Saturday\"])\n",
        "                        g_vc.add((DayWeek, RDF.type, BTP.DayWeek))\n",
        "                        g_vc.add((Period, BTP.onDay, DayWeek))\n",
        "                    case 'domenica':\n",
        "                        DayWeek = URIRef(BTP[\"Sunday\"])\n",
        "                        g_vc.add((DayWeek, RDF.type, BTP.DayWeek))\n",
        "                        g_vc.add((Period, BTP.onDay, DayWeek))\n",
        "                    case _:\n",
        "                        # No day provided\n",
        "                        pass\n",
        "        pbar.update(chunksize)\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 70 or len(threads) == psutil.cpu_count():\n",
        "            # I'm using more than 70% of the whole RAM or I'm using all the threads\n",
        "            for thread in threads:\n",
        "                thread.join()\n",
        "            threads = []\n",
        "\n",
        "        # Save the graph in a new thread\n",
        "        thread = threading.Thread(target=save_graph, args=(g_vc, '/content/vehicle_count_populated_'+year_dataset+'_'+str(piece)+'.ttl'))\n",
        "        thread.start()\n",
        "        # Ready for the next piece\n",
        "        piece += 1\n",
        "\n",
        "        # Free memory\n",
        "        del g_vc\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wait for all the threads to finish\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "print(\"--- merging vehicle count ---\")\n",
        "\n",
        "merged_graph = Graph()\n",
        "\n",
        "regex = re.compile(r'vehicle_count_populated_\\d{4}_\\d+\\.ttl')\n",
        "\n",
        "files = []\n",
        "\n",
        "for filename in os.listdir(save_path):\n",
        "    if regex.match(filename):\n",
        "        files.append(filename)\n",
        "\n",
        "pbar = tqdm(total=len(files))\n",
        "\n",
        "for filename in files:\n",
        "    merged_graph.parse(os.path.join('/content/', filename), format='turtle')\n",
        "    pbar.update(1)\n",
        "\n",
        "# Save the merged graph\n",
        "merged_graph.serialize(destination=os.path.join(save_path, 'vehicle_count_populated.ttl'), format='turtle')\n",
        "\n",
        "# Remove all the partial files in the directory\n",
        "for filename in os.listdir(save_path):\n",
        "    if regex.match(filename):\n",
        "        os.remove(os.path.join(save_path, filename))"
      ],
      "metadata": {
        "id": "v5t6DxfI3NIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzahuoyABmae"
      },
      "outputs": [],
      "source": [
        "print(\"--- populating vehicle accuracy ---\")\n",
        "\n",
        "# Load coils dataset\n",
        "g_coils = Graph()\n",
        "g_coils.bind(\"xsd\", XSD)\n",
        "g_coils.parse(os.path.join(save_path, 'coils_populated.ttl'), format='turtle')\n",
        "\n",
        "\n",
        "# FOR TEST TRY TO USE THE COILS DATASET!!\n",
        "# g_coils.parse(os.path.join(save_path, 'spire_populated.ttl'), format='turtle')\n",
        "\n",
        "\n",
        "# Query to get the coil's code associated to an ID (input: ID)\n",
        "code_coil_query = prepareQuery(\"\"\"\n",
        "    SELECT DISTINCT ?coil WHERE {\n",
        "        ?coil btp:hasID ?id .\n",
        "    FILTER (?id = ?coil_id)\n",
        "                               }\"\"\" , initNs={\"btp\": BTP})\n",
        "\n",
        "for namefile in accuratezza_spire:\n",
        "\n",
        "    year_dataset = namefile.split('/')[-1].split('_')[2].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Graph\n",
        "        g_acc = Graph()\n",
        "\n",
        "        # Bind Namespaces\n",
        "        g_acc.bind(\"xsd\", XSD)\n",
        "        g_acc.bind(\"btp\", BTP)\n",
        "\n",
        "        for index, row in chunk.iterrows():\n",
        "\n",
        "            for i in range(2, 26):\n",
        "\n",
        "                ## VEHICLEDETECTION:\n",
        "                # -uri: vehicleDetection_ + id number + _ + date.\n",
        "                # -attributi: hasAccuracy, and hasCount.\n",
        "\n",
        "                coil = ''\n",
        "\n",
        "                # Query to get the coil's code associated to an ID\n",
        "                res = g_coils.query(code_coil_query, initBindings={'coil_id':Literal(row['codice spira'], datatype=XSD.string)})\n",
        "                if res == [] or res == None:\n",
        "                    # I skip the record -> next record\n",
        "                    continue\n",
        "                else:\n",
        "                    for r in res:\n",
        "                        coil = str(r.coil).replace('http://www.dei.unipd.it/~gdb/ontology/btp/#coil_', '')\n",
        "                        # It must be one!\n",
        "                        break\n",
        "\n",
        "                date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "\n",
        "                VehicleDetection = URIRef(BTP[\"vehicleDetection_\"+coil+\"_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+str(i-2).zfill(2)+\":00-\"+str(i-1).zfill(2)+\":00\"])\n",
        "                g_acc.add((VehicleDetection, RDF.type, BTP.VehicleDetection))\n",
        "                percentage = row.iloc[i].replace('%', '')\n",
        "                g_acc.add((VehicleDetection, BTP.hasAccuracy, Literal(float(percentage), datatype=XSD.float)))\n",
        "\n",
        "                # # PERIOD:\n",
        "                # -uri: period_ + date + _ + hour1 + _ + hour2.\n",
        "                # -attributi: startTime and endTime.\n",
        "                # -object properties: onDay.\n",
        "\n",
        "                Period = URIRef(BTP[\"period_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+str(i-2).zfill(2)+\":00-\"+str(i-1).zfill(2)+\":00\"])\n",
        "                g_acc.add((Period, RDF.type, BTP.Period))\n",
        "\n",
        "                g_acc.add((Period, BTP.isObservedOnPeriod, VehicleDetection))\n",
        "                g_acc.add((VehicleDetection, BTP.hasObservedOnPeriod, Period))\n",
        "\n",
        "                startTime = str(i-2).zfill(2)+\":00\"\n",
        "                date_obj = datetime.datetime.strptime(str(row['data']), '%Y-%m-%d')\n",
        "\n",
        "                g_acc.add((Period, BTP.startTime, Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+startTime, datatype=XSD.dateTime)))\n",
        "\n",
        "                endTime = str(i-1).zfill(2)+\":00\"\n",
        "\n",
        "                # If the endTime is 24 -> date+1 and endTime = 00\n",
        "                if(endTime == '24:00'):\n",
        "                    endTime = '00:00'\n",
        "                    # I add one day\n",
        "                    date_obj = date_obj + datetime.timedelta(days=1)\n",
        "\n",
        "                g_acc.add((Period, BTP.endTime, Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+endTime, datatype=XSD.dateTime)))\n",
        "\n",
        "        pbar.update(chunksize)\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 70 or len(threads) == psutil.cpu_count():\n",
        "            # I'm using more than 70% of the whole RAM or I'm using all the threads\n",
        "            for thread in threads:\n",
        "                thread.join()\n",
        "            threads = []\n",
        "\n",
        "        # Save the graph in a new thread\n",
        "        thread = threading.Thread(target=save_graph, args=(g_acc, '/content/vehicle_accuracy_populated_'+year_dataset+'_'+str(piece)+'.ttl'))\n",
        "        thread.start()\n",
        "        # Ready for the next piece\n",
        "        piece += 1\n",
        "\n",
        "        del g_acc\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaLLV0NCBxCl"
      },
      "outputs": [],
      "source": [
        "# Free memory\n",
        "del g_coils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wait for all the threads to finish\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "print(\"--- merging coils ---\")\n",
        "\n",
        "merged_graph = Graph()\n",
        "\n",
        "regex = re.compile(r'vehicle_accuracy_populated_\\d{4}_\\d+\\.ttl')\n",
        "\n",
        "files = []\n",
        "\n",
        "for filename in os.listdir(save_path):\n",
        "    if regex.match(filename):\n",
        "        files.append(filename)\n",
        "\n",
        "pbar = tqdm(total=len(files))\n",
        "\n",
        "for filename in files:\n",
        "    merged_graph.parse(os.path.join('/content/', filename), format='turtle')\n",
        "    pbar.update(1)\n",
        "\n",
        "# Save the merged graph\n",
        "merged_graph.serialize(destination=os.path.join(save_path, 'vehicle_accuracy_populated.ttl'), format='turtle')\n",
        "\n",
        "# Remove all the partial files in the directory\n",
        "for filename in os.listdir(save_path):\n",
        "    if regex.match(filename):\n",
        "        os.remove(os.path.join(save_path, filename))"
      ],
      "metadata": {
        "id": "cGi5EukH3Yjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj-HzvIKBzRk"
      },
      "outputs": [],
      "source": [
        "print(\"--- populating pollution data ---\")\n",
        "\n",
        "for namefile in centraline:\n",
        "\n",
        "    year_dataset = namefile.split('/')[-1].split('_')[2].split('.')[0]\n",
        "    piece = 0\n",
        "\n",
        "    total_rows = len(pd.read_csv(namefile))\n",
        "    pbar = tqdm(total=total_rows)\n",
        "\n",
        "    for chunk in pd.read_csv(namefile, sep=';', chunksize=chunksize):\n",
        "\n",
        "        # Graph\n",
        "        g_pol = Graph()\n",
        "\n",
        "        # Bind Namespaces\n",
        "        g_pol.bind(\"xsd\", XSD)\n",
        "        g_pol.bind(\"btp\", BTP)\n",
        "\n",
        "        for index, row in chunk.iterrows():\n",
        "\n",
        "            ## POLLUTIONSTATION:\n",
        "            # -uri: centralUnit + pollution name.\n",
        "            # -object properties: hasRegister, and isRegistered.\n",
        "\n",
        "            PollutionStation = URIRef(BTP[\"controlUnit\" + (str(row['COD_STAZ']).lower()).replace(\" \", \"\")])\n",
        "            g_pol.add((PollutionStation, RDF.type, BTP.PollutionStation))\n",
        "\n",
        "            # PERIOD:\n",
        "            # -uri: period_ + date + _ + hour1 + _ + hour2.\n",
        "            # -attributi: startTime and endTime.\n",
        "            # -object properties: onDay.\n",
        "\n",
        "            # date format: yyyy-mm-ddThh:mm:ss+hh:mm\n",
        "            # keep only the data: 'Thh:mm:ss+hh:mm' -> yyyy-mm-dd\n",
        "            date_obj = datetime.datetime.strptime((str(row['DATA_INIZIO']).split('T'))[0], '%Y-%m-%d')\n",
        "            # keep only the hour: 'Thh:mm:ss+hh:mm' -> hh:mm:ss\n",
        "            startTime = str((((str(row['DATA_INIZIO']).split('T'))[1].split('+')[0]).split(':'))[0])+\":00\"\n",
        "            endTime = str((((str(row['DATA_FINE']).split('T'))[1].split('+')[0]).split(':'))[0])+\":00\"\n",
        "            Period = URIRef(BTP[\"period_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+startTime+\"-\"+endTime])\n",
        "\n",
        "            g_pol.add((Period, RDF.type, BTP.Period))\n",
        "            g_pol.add((Period, BTP.startTime, Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+startTime, datatype=XSD.dateTime)))\n",
        "            g_pol.add((Period, BTP.endTime, Literal(date_obj.strftime('%Y-%m-%d')+\"T\"+endTime, datatype=XSD.dateTime)))\n",
        "\n",
        "            ## CHEMICALDETECTION:\n",
        "            # -uri: chemicalDetection_ + pollution_station_name + _ + date + _ + element.\n",
        "            # -attributi: inQuantity (conversion all in ug/m), and hasChemicalName.\n",
        "            # -object properties: isDetectedOnPeriod, hasDetectedOnPeriod, hasDetect, and isDetected.\n",
        "\n",
        "            chemical_element = (row['AGENTE'].split(\"(\")[0]).strip()\n",
        "            ChemicalElement = URIRef(BTP[\"chemicalElement_\"+chemical_element])\n",
        "\n",
        "            date_obj = datetime.datetime.strptime((str(row['DATA_INIZIO']).split('T'))[0], '%Y-%m-%d')\n",
        "            ChemicalDetection = URIRef(BTP[\"chemicalDetection_\"+(str(row['COD_STAZ']).lower()).replace(\" \", \"\")+\"_\"+date_obj.strftime('%Y-%m-%d')+\"_\"+startTime+\"-\"+endTime+\"_\"+chemical_element])\n",
        "            g_pol.add((ChemicalDetection, RDF.type, BTP.ChemicalDetection))\n",
        "\n",
        "            # Cast from mg/m^3 to ug/m^3\n",
        "            if(row['UM'] == 'mg/m3'):\n",
        "                g_pol.add((ChemicalDetection, BTP.inQuantity, Literal((row['VALORE']*1000), datatype=XSD.float)))\n",
        "            else:\n",
        "                g_pol.add((ChemicalDetection, BTP.inQuantity, Literal((row['VALORE']), datatype=XSD.float)))\n",
        "\n",
        "            ## CHEMICALELEMENT:\n",
        "            # -uri: chemicalElement_ + chemical element name.\n",
        "            # -object properties: hasDetect, and isDetected\n",
        "\n",
        "            g_pol.add((ChemicalElement, RDF.type, BTP.ChemicalElement))\n",
        "            g_pol.add((ChemicalDetection, BTP.hasDetect, ChemicalElement))\n",
        "            g_pol.add((ChemicalElement, BTP.isDetected, ChemicalDetection))\n",
        "\n",
        "            if len(row['AGENTE'].split(\"(\")) > 1:\n",
        "                chemical_element_name = (((row['AGENTE'].split(\"(\")[1]).replace(\")\",\"\")).strip()).lower()\n",
        "\n",
        "                match chemical_element_name:\n",
        "                    case 'benzene':\n",
        "                        g_pol.add((ChemicalDetection, BTP.hasChemicalName, Literal(\"Benzene\", datatype=XSD.string)))\n",
        "                    case 'monossido di carbonio':\n",
        "                        g_pol.add((ChemicalDetection, BTP.hasChemicalName, Literal(\"Carbon monoxide\", datatype=XSD.string)))\n",
        "                    case 'monossido di azoto':\n",
        "                        g_pol.add((ChemicalDetection, BTP.hasChemicalName, Literal(\"Nitrogen Monoxide\", datatype=XSD.string)))\n",
        "                    case 'biossido di azoto':\n",
        "                        g_pol.add((ChemicalDetection, BTP.hasChemicalName, Literal(\"Nitrogen dioxide\", datatype=XSD.string)))\n",
        "                    case 'ossidi di azoto':\n",
        "                        g_pol.add((ChemicalDetection, BTP.hasChemicalName, Literal(\"Nitrogen oxides\", datatype=XSD.string)))\n",
        "                    case 'ozono':\n",
        "                        g_pol.add((ChemicalDetection, BTP.hasChemicalName, Literal(\"Ozone\", datatype=XSD.string)))\n",
        "                    case _:\n",
        "                        # New element provided\n",
        "                        g_pol.add((ChemicalDetection, BTP.hasChemicalName, Literal(chemical_element_name, datatype=XSD.string)))\n",
        "\n",
        "        pbar.update(chunksize)\n",
        "\n",
        "        # Memory monitor\n",
        "        if psutil.virtual_memory().percent > 70 or len(threads) == psutil.cpu_count():\n",
        "            # I'm using more than 70% of the whole RAM or I'm using all the threads\n",
        "            for thread in threads:\n",
        "                thread.join()\n",
        "            threads = []\n",
        "\n",
        "        # Save the graph in a new thread\n",
        "        thread = threading.Thread(target=save_graph, args=(g_pol, 'content/pollution_station_populated_'+year_dataset+'_'+str(piece)+'.ttl'))\n",
        "        thread.start()\n",
        "        # Ready for the next piece\n",
        "        piece += 1\n",
        "\n",
        "        del g_pol\n",
        "\n",
        "    pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Wait for all the threads to finish\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "print(\"--- merging coils ---\")\n",
        "\n",
        "merged_graph = Graph()\n",
        "\n",
        "regex = re.compile(r'pollution_station_populated_\\d{4}_\\d+\\.ttl')\n",
        "\n",
        "files = []\n",
        "\n",
        "for filename in os.listdir(save_path):\n",
        "    if regex.match(filename):\n",
        "        files.append(filename)\n",
        "\n",
        "pbar = tqdm(total=len(files))\n",
        "\n",
        "for filename in files:\n",
        "    merged_graph.parse(os.path.join('/content/', filename), format='turtle')\n",
        "    pbar.update(1)\n",
        "\n",
        "# Save the merged graph\n",
        "merged_graph.serialize(destination=os.path.join(save_path, 'pollution_station_populated.ttl'), format='turtle')\n",
        "\n",
        "# Remove all the partial files in the directory\n",
        "for filename in os.listdir(save_path):\n",
        "    if regex.match(filename):\n",
        "        os.remove(os.path.join(save_path, filename))"
      ],
      "metadata": {
        "id": "jbwmAHxn3hz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFyRldbxB8sL"
      },
      "outputs": [],
      "source": [
        "print(\"--- end ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}